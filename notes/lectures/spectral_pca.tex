% !TEX root = ../main.tex
\section{Spectral Theorem and PCA}

\subsection{Spectral Theorem}

\begin{theorem}[Spectral Theorem]
    Let $A \in \mathbb{R}^{n \times n}$ be a \textbf{symmetric} matrix.
    Then there is an \textbf{orthonormal basis} of $\mathbb{R}^{n}$ composed of eigenvectors of $A$.
\end{theorem}

That means if $A$ is symmetric, then there exists an orthonormal basis $(v_{1},\dots,v_{n})$ and $\lambda_{1},\dots, \lambda_{n} \in \mathbb{R}$ such that:
\begin{align}
Av_{i}&=\lambda_{i}v_{i} \quad \forall i \in \{ 1,\dots,n \} \\
AV&=V\Lambda \tag{$\Lambda=\text{diag}(\lambda_{1},\dots,\lambda_{n})$} \\
A&=V\Lambda V^{\top} \tag{Since $V^{\top}=V^{-1}$} \\
A&=\sum_{i=1}^{n}\lambda_{i}v_{i}v_{i}^{\top}
\end{align}
Geometrically, $\lambda_{i}v_{i}v_{i}^{\top}$ is the orthogonal projection onto $v_{i}$ scaled by $\lambda_i$.

\begin{theorem}[Matrix Formulation]
    Let $A \in \mathbb{R}^{n \times n}$ be a \textbf{symmetric} matrix.
    Then there exists an orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^{\top}$.
\end{theorem}

\subsubsection{Why are eigenvectors orthogonal?}
Let $v_1, v_2$ be eigenvectors for distinct eigenvalues $\lambda_1, \lambda_2$.
\begin{align}
\lambda_1 \langle v_1, v_2 \rangle &= \langle Av_1, v_2 \rangle \\
&= \langle v_1, A^{\top}v_2 \rangle \\
&= \langle v_1, Av_2 \rangle \tag{Symmetry $A=A^{\top}$} \\
&= \langle v_1, \lambda_2 v_2 \rangle \\
&= \lambda_2 \langle v_1, v_2 \rangle
\end{align}
$(\lambda_1 - \lambda_2)\langle v_1, v_2 \rangle = 0$. Since $\lambda_1 \neq \lambda_2$, $\langle v_1, v_2 \rangle = 0$.

\subsection{Principal Component Analysis (PCA)}

\subsubsection{Empirical mean and covariance}
Consider a dataset $a_{1},\dots,a_{n} \in \mathbb{R}^{d}$.
\begin{itemize}
    \item \textbf{Mean:} $\mu=\frac{1}{n}\sum_{i=1}^{n}a_{i}$.
    \item \textbf{Covariance Matrix:} $\Sigma = \frac{1}{n}\sum_{i=1}^{n}(a_{i}-\mu)(a_{i}-\mu)^{\top}$.
\end{itemize}
It describes the variance in all possible unit directions.

\subsubsection{PCA Algorithm}
\begin{goal}
Find a lower dimensional representation $\tilde{a}_{1},\dots\tilde{a}_{n} \in \mathbb{R}^{k}$ where $k \ll d$.
\end{goal}

Assume centered data ($\mu=0$).
Then $\Sigma = \frac{1}{n} \sum a_i a_i^{\top} = \frac{1}{n} A^{\top}A$, which is symmetric. We can apply the Spectral Theorem.

\begin{idea}
    \textbf{Direction of maximal variance:}
    We aim to find $u$ where variance is maximal: $\max_{\lVert u \rVert = 1} u^{\top}\Sigma u$.
    This corresponds to the eigenvector associated with the largest eigenvalue of the covariance matrix.
\end{idea}

The $j$-th direction of maximal variance is $v_{j}$, the solution to maximizing $v^{\top}\Sigma v$ subject to $\lVert v \rVert=1$ and $v \perp v_1, \dots, v \perp v_{j-1}$. This is the $j$-th eigenvector.

The dimensionally reduced dataset in $k$ dimensions is:
\[ \begin{pmatrix} \langle v_{1},a_{1}  \rangle \\ \vdots \\ \langle v_{k},a_{1}  \rangle \end{pmatrix}, \dots, \begin{pmatrix} \langle v_{1},a_{n}  \rangle \\ \vdots \\ \langle v_{k},a_{n}  \rangle \end{pmatrix} \]