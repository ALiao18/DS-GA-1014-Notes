% !TEX root = ../main.tex
\section{SVD and Linear Algebra for Graphs}

\subsection{Singular Value Decomposition (SVD)}

\begin{quote}
"This is the single most powerful matrix decomposition, and once you know it, well, there's a before and after." - Florentein Guth
\end{quote}

\begin{goal}
\textbf{Motivation:} How do we generalize the spectral theorem decomposition to generic matrices $A \in \mathbb{R}^{n\times m}$?
Since $A$ is now rectangular, $Av_{i}=\lambda_{i}v_{i}$ no longer has any meaning.
\end{goal}

\begin{idea}
Let $Au_{i}=\sigma_{i}v_{i}$ for another orthonormal family $v_{1},\dots,v_{m}$.
So, we know $A$ is an operation from $\mathbb{R}^{m} \to \mathbb{R}^{n}$ such that $Av_{i}=\sigma_{i}u_{i}$.
\end{idea}

\textbf{Derivation:}
$A^{\top}Av_i = \sigma_i A^{\top}u_i$. From the transpose, $A^{\top}u_i = \sigma_i v_i$.
Thus $A^{\top}Av_i = \sigma_i^2 v_i$.
\begin{itemize}
    \item $v_i$ are eigenvectors of $A^{\top}A$.
    \item $\sigma_i = \sqrt{\lambda_i(A^{\top}A)}$.
    \item $u_i = \frac{Av_i}{\sigma_i}$.
\end{itemize}

\begin{theorem}[SVD]
    Any matrix $A \in \mathbb{R}^{n \times m}$ can be written as $A=U\Sigma V^{\top}$.
\end{theorem}

\subsection{Graphs and Graph Laplacian}
Let $G=(V, E)$ be a graph with $n$ vertices.
\begin{itemize}
    \item \textbf{Adjacency Matrix} $A$: $A_{ij}=1$ if $(i,j) \in E$, else 0.
    \item \textbf{Degree Matrix} $D$: Diagonal matrix with $D_{ii} = \deg(i)$.
    \item \textbf{Laplacian} $L$: $L = D - A$.
\end{itemize}

\subsubsection{Properties of L}
\begin{enumerate}
    \item $L$ is symmetric.
    \item $L$ is Positive Semi-Definite (PSD).
    \item $\forall x \in \mathbb{R}^n, x^{\top}Lx = \sum_{(i,j) \in E} (x_i - x_j)^2$.
    \item $Ker(L)$ contains the vector $\mathbf{1}$ (all ones), since sum of rows is 0.
\end{enumerate}

\begin{theorem}[Algebraic Connectivity]
    The multiplicity of the eigenvalue 0 of $L$ is equal to the number of connected components of $G$.
    $G$ is connected $\iff \lambda_{2} > 0$.
    $\lambda_{2}$ is known as the \textbf{algebraic connectivity}.
\end{theorem}

\subsection{Application: Spectral Clustering}

\subsubsection{Two clusters: Minimal cut problem}
\begin{definition}[Cut]
    The cut of $S \subset \{ 1,\dots,n \}$, denoted $cut(S)$, is the number of edges between $S$ and $S^C$.
\end{definition}

\begin{goal}
Find 2 clusters ($S, S^C$) such they are balanced and minimize edges across.
\end{goal}

Using sign vectors $x \in \{+1, -1\}^n$, we find $cut(S) = \frac{1}{4}x^{\top}Lx$.
Minimizing $x^{\top}Lx$ subject to $x \in \{-1, 1\}^n$ and $x \perp 1$ is NP-hard.

\begin{idea}
Relax the constraints to $v \in \mathbb{R}^n$ with $\lVert v \rVert^2 = n$.
This becomes the problem of finding the eigenvector for the second smallest eigenvalue, $v_2$.
\end{idea}

\textbf{Algorithm (2 Clusters):}
\begin{enumerate}
    \item Compute $v_{2}$, the second eigenvector of $L$.
    \item Set $x_{i}=v_{2}(i)$.
    \item Define $S=\{ i \mid v_{2}(i) \geq 0 \}$, $S^C=\{ i \mid v_{2}(i) < 0 \}$.
\end{enumerate}

\subsubsection{Spectral Clustering: k clusters}
\begin{enumerate}
    \item Compute $v_1, \dots, v_k$, the first $k$ eigenvectors of $L$.
    \item Embed every node $i$ into $\mathbb{R}^k$ using the rows of $V = [v_1, \dots, v_k]$.
    \item Run K-means clustering on these points in $\mathbb{R}^k$.
\end{enumerate}