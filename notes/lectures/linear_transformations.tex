% !TEX root = ../main.tex
\section{Linear Transformations and Matrices}

\subsection{Review}
There are 2 interpretations of vectors: geometric and numerical. These two interpretations \textit{bridge the abstract world and algorithmic world}.
\begin{itemize}
    \item The \textit{geometric world} provides the vectors, which contain scale and directionality.
    \item The \textit{numerical world} provides the basis, which are the coordinates to project the vectors to.
    \begin{itemize}
        \item Let basis be $e_{1},e_{2},\dots,e_{n}$.
        \item $\vec{v} =v_{1}e_{1}+v_{2}e_{2}+\dots+v_{n}e_{n}$.
    \end{itemize}
\end{itemize}

In $\mathbb{R}^n$ with the canonical basis, it is trivial because $\vec{v}=(v_{1},\dots,v_{n})=v_{1}\vec{e_{1}}+\dots+v_{n}\vec{e_{n}}$.
However, when the vectors are functions, since they don't live in $\mathbb{R}^n$, they don't really have components and the $\mathbb{R}^n$ vector geometric interpretation is invalid.
However, we can still describe the coordinates of a function in a given basis.
It is key to distinguish the two.

\subsection{Linear Transformations}
\textit{Geometric interpretation:} operations done on vectors with linear properties.
\textit{Numerical interpretation:} arrays of numbers, aka matrices.

\subsubsection{Definition}
Symmetries and rotations are linear mappings. So they are functions from vectors to vectors.
\begin{align}
L:\quad\mathbb{R}^2 & \to\quad\mathbb{R}^2 \\
v & \mapsto\quad L(v)
\end{align}

\begin{definition}[Linear Transformation]
    A function $L: \mathbb{R}^m \to \mathbb{R}^n$ is linear $\iff$
    \begin{enumerate}
        \item Closed under addition: $\forall v, w \in \mathbb{R}^m, L(v+w)=L(v)+L(w)$
        \item Closed under scalar multiplication: $\forall v \in \mathbb{R}^m, \alpha \in \mathbb{R}, L(\alpha v)=\alpha L(v)$
    \end{enumerate}
\end{definition}

\noindent Note:
\begin{enumerate}
    \item They don't have to be in the same vector space!
    \item They are transformations that comply with closure under linear combinations.
\end{enumerate}

\subsubsection*{Examples}

\textbf{1. $\mathbb{R}^2 \to \mathbb{R}^3$ linear transformation:}
\begin{align}
L:\quad\mathbb{R}^2 & \to\quad\mathbb{R}^3 \\
(v_{1}, v_{2}) & \mapsto\quad (5v_{1}, 0, v_{1}+v_{2})
\end{align}

\begin{proof}
    \textbf{Closure under addition:} Let $(v_{1}, v_{2}), (w_{1},w_{2}) \in \mathbb{R}^2$.
    \begin{align}
    L(v+w)&=L((v_{1}+w_{1}), (v_{2}+w_{2})) \\
    &=(5(v_{1}+w_{1}), 0, (v_{1}+w_{1})+(v_{2}+w_{2})) \\
    &=(5v_{1},0,v_{1}+v_{2})+(5w_{1},0,w_{1}+w_{2}) \\
    &=L(v)+L(w)
    \end{align}
    \textbf{Closure under scalar multiplication:} Let $(v_{1},v_{2}) \in \mathbb{R}^2, \lambda \in \mathbb{R}$.
    \begin{align}
    L(\lambda v) &= L(\lambda v_{1}, \lambda v_{2}) \\
    &= (5\lambda v_{1}, 0, \lambda(v_{1}+v_{2})) \\
    &=\lambda(5v_{1}, 0, (v_{1}+v_{2}))  \\
    &=\lambda L(v)
    \end{align}
    $\therefore L$ is a linear transformation.
\end{proof}

\textbf{2. $\mathbb{R} \to \mathbb{R}$ non-linear transformation:}
\begin{align}
L:\quad \mathbb{R} &\to \mathbb{R} \\
x &\mapsto\quad x^2
\end{align}

\begin{proof}
    Using a counter example to demonstrate the square of the sums is not equal to the sum of the squares:
    \begin{align}
    L(1+2)&=(1+2)^2 = 9 \\
    L(1)+L(2)&=1+4 = 5
    \end{align}
    $5 \neq 9$, therefore $L$ is not a linear transformation.
\end{proof}

\subsection{Matrices}
\begin{definition}[Matrix]
    An $n \times m$ matrix is an array with $n$ rows and $m$ columns. We denote by $\mathbb{R}^{n \times m}$ the set of all $n \times m$ matrices.
\end{definition}

\begin{definition}[Canonical Matrix]
    We can encode a linear map $L: \mathbb{R}^m \to \mathbb{R}^n$ by a $n \times m$ matrix.
    \begin{align}
    \tilde{L} = \begin{pmatrix}
    L_{1,1} & L_{1,2} & \cdots & L_{1,m} \\
    L_{2,1} & L_{2,2} & \cdots & L_{2,m} \\
    \vdots & \vdots & \ddots & \vdots \\
    L_{n,1} & L_{n,2} & \cdots & L_{n,m} \\
    \end{pmatrix}
    \end{align}
    Where we write $L(e_{j}) = (L_{1,j}, L_{2,j}, \dots, L_{n,j})^{\top}$.
\end{definition}

\subsubsection*{Examples}
\textbf{Identity matrix $I$:}
\begin{align}
\tilde{L} = \begin{pmatrix} | & | \\ L(1,0) & L(0, 1) \\ | & | \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\end{align}

\subsection{Kernels and Images}

\begin{definition}[Kernel]
    The \textbf{kernel} (or nullspace) of $L$ is the set of all vectors $v \in \mathbb{R}^{m}$ such that $L(v)=0$.
    \[ Ker(L):=\{ v \in \mathbb{R}^{m} \mid L(v)=0\} \]
    $Ker(L)$ is a subspace of $\\mathbb{R}^{m}$.
\end{definition}

\begin{definition}[Image]
    The \textbf{image} (aka range, column space) of $L$ is the set of all vectors $v \in \mathbb{R}^{n}$ such that there exists $v \in \mathbb{R}^{m}$ such that $L(v)=v$.
    \[ Im(L)=\{ L(v)\mid v \in \mathbb{R}^{m} \} \subseteq \mathbb{R}^{n} \]
    $\mathrm{Im}(L)$ is a subspace of $\mathbb{R}^{n}$.
\end{definition}

\subsection{Application to ML}

\begin{goal}
Given data of input-output pairs $\{ (x_{1},y_{1}),\dots,(x_{n},y_{n}) \}$ where $x_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}$, predict $y$ for a new $x$.
\end{goal}

\begin{idea}
\textbf{Hypothesis:} $\exists (\theta_{1},\dots,\theta_{m}) \text{ s.t. } \theta_{1}x_{i,1}+\dots+\theta_{m}x_{i,m}=y_{i}$.
\end{idea}

\noindent \textbf{Problem:} How do we find $\theta$? Do we need more data?
$\exists \theta \text{ s.t. } X\theta=y$?
\begin{itemize}
    \item Yes $\iff y \in \mathrm{Im}(X)$.
    \item If $y \notin \mathrm{Im}(X)$: no solution.
    \item If $y \in \mathrm{Im}(X)$: at least 1 solution.
\end{itemize}

If $\theta$ is a solution, $X\theta=y$. Let $\theta_0$ be a particular solution, then $X\theta_0 = y$.
\[ X\theta - X\theta_0 = y - y = 0 \implies X(\theta - \theta_0) = 0 \]
Therefore $\theta - \theta_0 \in Ker(X)$.
The general solution is $\{ \theta_0 + v, v \in Ker(X) \} = \theta_0 + Ker(X)$.
\begin{itemize}
    \item If $Ker(X) = \{0\}$: one unique solution $\theta_0$.
    \item If $Ker(X) \neq \{0\}$: infinite solutions.
\end{itemize}