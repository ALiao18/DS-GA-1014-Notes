% !TEX root = ../main.tex
\section{Norms, Inner Products, and Orthogonality}

\subsection{Norms}

\begin{definition}[Euclidean Norm]
    We define the Euclidean norm of $x=(x_{1},x_{2},\dots, x_{n}) \in \mathbb{R}^{n}$ as:
    \[ \lVert x \rVert_{2}=\sqrt{ x_{1}^{2}+\dots+x_{n}^{2} } \]
\end{definition}

\subsubsection{General norms}
For it to be a measure of length, it must satisfy 3 qualities.
Let $V$ be a vector space.

\begin{definition}[Norm]
    A \textbf{norm} $\lVert \cdot \rVert$ on $V$ is a function from $V$ to $\mathbb{R}_{\geq 0}$ that verifies:
    \begin{enumerate}
        \item \textbf{Homogeneity:} $\lVert \alpha v \rVert = \lvert \alpha \rvert \times \lVert v \rVert \quad \forall\alpha \in \mathbb{R}, v \in V$.
        \item \textbf{Positive definiteness:} If $\lVert x \rVert=0$ for some $x \in V$, then $x=0$.
        \item \textbf{Triangular inequality:} $\lVert u+v \rVert \leq \lVert u \rVert + \lVert v \rVert \quad \forall u, v \in V$.
    \end{enumerate}
\end{definition}

\textbf{Other norms:}
\begin{itemize}
    \item $\ell_1$ norm (Manhattan): $\lVert x \rVert_{1}=\sum_{i=1}^{n}\lvert x_{i} \rvert$.
    \item $\ell_{\infty}$ norm: $\lVert x \rVert_{\infty}=\max(\lvert x_{1}\rvert, \dots, \lvert x_{n} \rvert)$.
    \item $\ell_{p}$ norm: $\lVert x \rVert_{p}=\left(\sum_{i=1}^{n} \lvert x_{i} \rvert^p\right)^{\frac{1}{p}}$.
\end{itemize}

\subsubsection*{Exercise}
For each of the norms $\lVert \cdot \rVert_1, \lVert \cdot \rVert_2 , \lVert \cdot \rVert_{\infty}$, draw the "sphere" $S=\{ x \in \mathbb{R}^{2} \mid \lVert x \rVert =1\}$.
\begin{enumerate}
    \item \textbf{$\ell_1$ Norm:} The 4 vectors are $(0, 1), (1, 0), (0, -1), (-1, 0)$. Connecting these 4 vectors forms a square ($|x_1| + |x_2| = 1$).
    \item \textbf{$\ell_2$ Norm:} Forms the standard circle ($x_1^2 + x_2^2 = 1$).
    \item \textbf{$\ell_{\infty}$ Norm:} Forms a square that envelops both $\ell_1$ and $\ell_2$. ($\max(|x_1|, |x_2|) = 1$).
\end{enumerate}

\subsubsection{Application to regularized linear regression}
Suppose this is an under-constrained (over-parameterized) solution.
Thus the solution occupies an affine space $\{ \theta \mid X\theta=y \}$.

\begin{idea}
    How do you pick which $\theta$ if there are infinite solutions?
    By picking the one with the smallest norm.
\end{idea}
\[ \text{minimize } \lVert \theta \rVert \text{ such that } X\theta=y \]
Different norms induce different solutions. For $\ell_1$, we prefer solutions on the axes (sparse solutions). For $\ell_2$, we prefer solutions distributed across components.

\subsection{Inner Products}

\begin{definition}[Inner Product]
    Let $V$ be a vector space. An \textbf{inner product} on $V$ is a function $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ that satisfies:
    \begin{enumerate}
        \item \textbf{Linearity in the first argument:} $\langle \alpha x + \beta y, z \rangle = \alpha \langle x,z \rangle + \beta \langle y,z \rangle$.
        \item \textbf{Symmetry:} $\langle v,w \rangle = \langle w,v \rangle$.
        \item \textbf{Positive definiteness:} $\langle v,v \rangle \geq 0$ with equality iff $v=0$.
    \end{enumerate}
\end{definition}

\begin{proposition}[Induced norm]
    If $\langle \cdot,\cdot \rangle$ is an inner product on $V$ then,
    \[ \lVert v \rVert \stackrel{\text{def}}{=} \sqrt{ \langle v,v \rangle } \]
    is a norm on $V$. We say that the norm $\lVert \cdot \rVert$ is induced by the inner product.
\end{proposition}

\subsection{Orthogonality}

\begin{definition}[Orthogonality]
    Two vectors $x,y$ are orthogonal ($x \perp y$) if $\langle x,y \rangle=0$.
    We say $x$ is orthogonal to a set $S$ if $x \perp v$ for all $v \in S$.
\end{definition}

\begin{theorem}[Pythagorean Theorem]
    If $x \perp y$, then:
    \[ \lVert x+y \rVert^{2}=\lVert x \rVert^{2}+\lVert y \rVert^{2} \]
\end{theorem}

\begin{proof}
\begin{align}
\lVert x+y \rVert^{2}&=\langle x+y, x+y \rangle \\
                     &=\langle x,x  \rangle+\langle y,x  \rangle+\langle x,y  \rangle + \langle y,y  \rangle \\
                     &= \lVert x \rVert^{2} + \lvert y \rvert^{2} + 2\langle x,y  \rangle   \\
                     &= \lVert x \rVert^{2} + \lvert y \rvert^{2} \iff x \perp y
\end{align}
\end{proof}

\subsubsection{Orthogonal Projection}
Let $S$ be a subspace of $\mathbb{R}^{n}$.
The \textbf{orthogonal projection} of $x$ onto $S$ is the vector $P_{S}(x)$ in $S$ that minimizes the distance to $x$:
\[ P_{S}(x) \stackrel{\text{def}}{=} \underset{y \in S}{\operatorname{argmin}}\lVert x-y \rVert \]

\begin{proposition}
    Let $(v_{1},\dots,v_{k})$ be an orthonormal basis of $S$.
    Then for all $x \in \mathbb{R}^{n}$:
    \[ P_{S}(x)=\langle v_{1},x  \rangle v_{1}+\dots+\langle v_{k},x  \rangle v_{k} \]
    Let $V = \begin{pmatrix} | & & | \\ v_1 & \dots & v_k \\ | & & | \end{pmatrix}$ gather the orthonormal basis vectors of $S$.
    Then $P_{S}(x)=VV^{\top}x$.
\end{proposition}
\begin{enumerate}
    \item $P_S$ is a linear transform.
    \item $VV^{\top}$ is its matrix.
    \item $x - P_S(x) \perp S$.
\end{enumerate}