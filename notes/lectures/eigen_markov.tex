% !TEX root = ../main.tex
\section{Eigenvalues and Markov Chains}

\subsection{Eigenvalues and Eigenvectors}

\begin{definition}[Eigenvalues and Eigenvectors]
    Let $A \in \mathbb{R}^{n \times n}$.
    A non-zero vector $v \in \mathbb{R}^{n}$ is an eigenvector if $\exists \lambda \in \mathbb{R}$ s.t. $Av=\lambda v$.
\end{definition}
 
\begin{proposition}[Eigenvalue Properties]
    \begin{enumerate} 
        \item $\alpha \lambda$ is an eigenvalue of $\alpha A$. The eigenvector is unchanged.
        \[ (\alpha A)x = \alpha (Ax) = \alpha (\lambda x) = (\alpha \lambda)x \]
        \item $\lambda + \alpha$ is an eigenvalue of $A+\alpha Id$. The eigenvector is unchanged.
        \[ (A+\alpha Id)x = Ax + \alpha x = \lambda x + \alpha x = (\lambda + \alpha)x \]
        \item $\lambda^k$ is an eigenvalue of $A^k$. The eigenvector is unchanged.
        \[ A^k x = A^{k-1}(Ax) = \lambda A^{k-1}x = \dots = \lambda^k x \]
        \item If $A$ is invertible, $\frac{1}{\lambda}$ is an eigenvalue of $A^{-1}$. (An invertible matrix cannot have a zero eigenvalue).
    \end{enumerate}
\end{proposition}

\begin{definition}[Eigenspace]
    The eigenspace $E_{\lambda}(A) = Ker(A-\lambda Id)$ is the set of all eigenvectors for $\lambda$ (plus the zero vector).
    The dimension of $E_{\lambda}(A)$ is the \textbf{multiplicity} of $\lambda$.
\end{definition}

\begin{definition}[Spectrum]
    The set of all eigenvalues is the \textbf{spectrum} $Sp(A)$.
\end{definition}

\begin{proposition}[Bounds on Spectrum]
    A $n \times n$ matrix $A$ admits at most $n$ distinct eigenvalues.
    If $\lambda_{1},\dots,\lambda_{k}$ are distinct eigenvalues with multiplicities $d_1, \dots, d_k$, then $\sum d_i \leq n$.
\end{proposition}

\subsection{Markov Chains}

\begin{goal}
\textbf{Motivation:} PageRank. How to order web pages by importance?
\begin{enumerate}
    \item \textbf{Naive attempt:} Rank according to incoming links.
    \item \textbf{Random Surfer:} Suppose someone clicks on a random link every time. The page they spend the most time on is likely the most interesting.
\end{enumerate}
\end{goal}

This defines a Markov chain with transition matrix:
\[ 
P_{i,j} = \begin{cases}
\frac{1}{\deg(j)} & \text{if } j \to i \\
0 & \text{otherwise}
\end{cases}
\]
Note: Columns sum to 1 (Stochastic matrix).
$x^{t+1}=Px^t \implies x^t=P^tx^0$.

\begin{definition}[Invariant Measure]
    We define $\mu$ as an \textbf{invariant measure} (or stationary distribution) if $\mu = P\mu$.
    This means $\mu$ is an eigenvector of $P$ associated with eigenvalue 1.
\end{definition}

\subsubsection{Example: Random Spectator (Google Matrix)}
Imagine a "random spectator" watching tennis:
\begin{itemize}
    \item At time $t$, the spectator believes player $j$ is the best ($X_t=j$).
    \item They pick a game of player $j$ uniformly at random.
    \item If $j$ wins, they still believe $j$ is best.
    \item If $i$ beat $j$, they change their mind to $X_{t+1}=i$.
    \item With probability $\alpha$, they pick a new favorite uniformly at random.
\end{itemize}
This defines a transition matrix $P$. We rank the players according to the stationary distribution $\mu$ of the matrix:
\[ M = (1-\alpha)P + \frac{\alpha}{N}J \]
Where $J$ is the matrix of all ones (teleportation).

\subsection{Perron-Frobenius Theorem}

\begin{theorem}[Perron-Frobenius]
    Let $P$ be a stochastic matrix such that there exists $k \geq 1$ where all entries of $P^k$ are strictly positive (Regular/Ergodic). Then:
    \begin{enumerate}
        \item 1 is an eigenvalue of $P$ with a unique eigenvector $\mu \in \nabla_{n}$ (probability simplex).
        \item The eigenvalue 1 has multiplicity 1.
        \item All other eigenvalues satisfy $|\lambda| < 1$.
        \item For any initial $x^0$, $P^tx^0 \to \mu$ as $t \to \infty$.
    \end{enumerate}
\end{theorem}