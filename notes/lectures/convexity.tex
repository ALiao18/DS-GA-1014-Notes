% !TEX root = ../main.tex
\section{Convexity}

\begin{goal}
\textbf{Motivation:} In machine learning, we often minimize functions $f(\theta)=Loss(data, model_{\theta})$.
For higher dimensions, we rely on local searches (derivatives). Local information gives global information when the function is \textit{convex}.
\end{goal}

\subsection{Functions of n Variables}

\subsubsection{Single Variable ($n=1$)}
We define the tangent using the derivative.
\textbf{First-Order Taylor Approximation:}
\[ f(x_{0}+h)=f(x_{0})+h\cdot f'(x_{0}) + o(h) \]

\subsubsection{General Case ($n$ Variables)}
$f: \mathbb{R}^n \rightarrow \mathbb{R}$. We define the gradient:
\[ \nabla f(x)=\left( \frac{\partial{f}}{\partial{x_{1}}}(x), \dots, \frac{\partial{f}}{\partial{x_{n}}}(x)\right) \]
Taylor approximation:
\[ f(x+h) \approx f(x) +\langle \nabla f(x),h  \rangle \]

\begin{definition}[Jacobian Matrix]
    For vector-valued functions $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^m$, the \textbf{Jacobian} $\mathbf{J}(x)$ is an $m \times n$ matrix where the $i$-th row is $\nabla f_i(x)$.
\end{definition}

\begin{definition}[Hessian Matrix]
    For scalar-valued functions $f: \mathbb{R}^n \rightarrow \mathbb{R}$, the \textbf{Hessian} $\nabla^2 f(x)$ is the $n \times n$ matrix of partial derivatives:
    \[ (\nabla^2 f(x))_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}(x) \]
\end{definition}

\subsection{Convexity Definitions}

\begin{definition}[Convex Function]
    A function $f: \mathbb{R}^n \to \mathbb{R}$ is \textbf{convex} if for all $x,y \in \mathbb{R}^n$ and $\alpha \in [0,1]$:
    \[ f(\alpha x + (1-\alpha)y) \leq \alpha f(x) + (1-\alpha)f(y) \]
\end{definition}

\begin{definition}[Strictly Convex]
    A function is \textbf{strictly convex} if the inequality is strict for $x \neq y$ and $\alpha \in (0,1)$.
\end{definition}
Intuitively, the function always curves upwards. $f$ is concave if $-f$ is convex.
\\ \\
\begin{proposition} 
    We say that: 
    \begin{enumerate}
        \item Linear maps are both convex and concave.
        \item Norms are convex (Triangle Inequality).
        \item Sum of convex functions is convex.
    \end{enumerate}
\end{proposition}

\subsection{Convexity and Derivatives}

\begin{proposition}[Tangents]
    A differentiable function $f$ is convex $\iff \forall x,y\in \mathbb{R}^n$:
    \[ f(y)\geq f(x)+\langle \nabla f(x),(y-x)  \rangle \]
    (The function is always above its tangent).
\end{proposition}

\begin{corollary}
    If $f$ is convex and differentiable: $x$ is a global minimizer $\iff \nabla f(x)=0$.
\end{corollary}

\begin{proposition}[Hessian Condition]
    Let $f$ be twice-differentiable. Then $f$ is convex $\iff \nabla^{2}f(x)$ is Positive Semi-Definite (PSD) for all $x$.
    For strict convexity, we require $\nabla^2 f(x)$ to be Positive Definite.
\end{proposition}

\subsection{Jensen's Inequality}

\begin{theorem}[Jensen's Inequality]
    Let $f$ be a convex function. For $x_i \in \mathbb{R}^n$ and weights $\alpha_i \geq 0$ summing to 1:
    \[ f\left( \sum \alpha_{i}x_{i} \right)\leq\sum \alpha_{i}f(x_{i}) \]
    For a random variable $X$ taking values in $\mathbb{R}^n$:
    \[ f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] \]
\end{theorem}
One result of Jensen's inequality is that the variance is always non-negative ($Var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \geq 0$ since $x^2$ is convex).

\subsubsection*{Example: Entropy}
Consider a random variable $X$ taking values in $\{1, \dots, k\}$. Let $p_i = P(X=i)$.
The entropy is defined as $H(X) = - \sum p_i \log(p_i) = \mathbb{E}[-\log(p(X))]$.
Since $-\log(x)$ is convex, we can apply Jensen's inequality to bound the entropy.