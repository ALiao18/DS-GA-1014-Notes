% !TEX root = ../main.tex
\section{Orthogonal Matrices}

\subsection*{Preliminaries}
Two useful formulas:
\begin{enumerate}
    \item Let $x, y \in \mathbb{R}^{n}$. Then $\langle x,y \rangle=x^{\top}y$.
    \item Let $A \in \mathbb{R}^{n \times m}, x \in \mathbb{R}^{m}, y \in \mathbb{R}^{n}$. Then $\langle Ax,y \rangle=\langle x,A^{\top}y \rangle$.
    This tells us that we can put symmetric matrices on either side and the answer stays the same.
\end{enumerate}

\subsection{Gram-Schmidt Algorithm}

\begin{goal}
\textbf{Purpose:} How to create an orthonormal basis?
The Gram-Schmidt process takes as:
\begin{itemize}
    \item \textbf{Input:} A linearly independent family $(x_{1},\dots,x_{k})$ of $\mathbb{R}^{n}$.
    \item \textbf{Output:} An orthonormal basis $v_{1},\dots,v_{k}$ of $Span(x_{1},\dots,x_{k})$.
\end{itemize}
\end{goal}

\begin{corollary}
    Every subspace of $\mathbb{R}^{n}$ admits an orthonormal basis.
\end{corollary}

We need to do 2 things: normalize each vector, and remove the projection of each vector onto every other vector (project onto the complement).

\begin{solution}
For example, let there be $x_{1}, x_{2}$.
\begin{align}
v_{1}&=\frac{x_{1}}{\lVert x_{1} \rVert } \\
\tilde{v_{2}}&=x_{2}-\langle x_{2},v_{1}  \rangle v_{1} \\
v_{2}&=\frac{\tilde{v_{2}}}{\lVert \tilde{v_{2}} \rVert }
\end{align}
We check orthogonality: $\langle \tilde{v_{2}},v_{1}  \rangle = \langle x_{2}, v_{1} \rangle - \langle x_{2},v_{1}  \rangle \langle v_{1}, v_{1} \rangle = 0$.
\end{solution}

\subsection{Orthogonal Matrices}

\begin{definition}[Orthogonal Matrix]
    A matrix $Q \in \mathbb{R}^{n \times n}$ is \textbf{orthogonal} if its columns form an orthonormal basis of $\mathbb{R}^{n}$.
    \[ Q^{\top}Q = I_n \]
\end{definition}

\subsubsection{Properties}
Let $Q$ be an orthogonal matrix.
\begin{enumerate}
    \item \textbf{Isometry (Norm preserving):} $\lVert Qx \rVert = \lVert x \rVert$ for all $x \in \mathbb{R}^n$.
    \item \textbf{Angle preserving:} $\langle Qx, Qy \rangle = \langle x, y \rangle$.
    \item \textbf{Orthonormal rows:} The rows of $Q$ also form an orthonormal basis ($QQ^{\top} = I_n$).
    \item \textbf{Inverse:} $Q^{-1} = Q^{\top}$.
\end{enumerate}

\begin{proposition}[Change of Basis]
    Let $(a_1, \dots, a_n)$ be an orthonormal basis of $\mathbb{R}^n$, and $A$ the matrix collecting these vectors.
    The coordinates of $x$ in the $(a_{1},\dots,a_{n})$ basis are given by $x'=A^{\top}x$.
\end{proposition}

\begin{idea}
Usually finding the orthonormal bases requires solving a linear system (finding the inverse).
The general formula is $x'=A^{-1}x$, but $A^{-1}=A^{\top} \iff$ $A$ is an orthogonal matrix.
\end{idea}

\subsection{Eigenvalues and Eigenvectors}

\subsubsection{Introduction}
\begin{itemize}
    \item \textbf{Numerical:} Let $A$ be a product of simpler matrices.
    \item \textbf{Geometric:} Suppose we have a linear transformation $L: V \to V$. Is there a basis of $V$ where the matrix is diagonal?
\end{itemize}

\begin{definition}[Eigenvector and Eigenvalue]
    Let $A \in \mathbb{R}^{n \times n}$. A non-zero vector $v \in \mathbb{R}^{n}$ is said to be an \textbf{eigenvector} of $A$ if $\exists \lambda \in \mathbb{R}$ such that $Av=\lambda v$. The scalar $\lambda$ is then called an \textbf{eigenvalue} of $A$.
\end{definition}

\subsubsection*{Examples}
\begin{enumerate}
    \item \textbf{Identity matrix:} Any non-zero vector $v$ is an eigenvector of $Id$ with $\lambda =1$.
    \item \textbf{Matrix with Kernel:} The eigenvectors for eigenvalue 0 are $Ker(A) \setminus \{ 0 \}$. If 0 is an eigenvalue, $A$ is not invertible.
\end{enumerate}

\subsubsection{Matrix with no eigenvalues}
Consider a rotation matrix $R_{\theta} = \begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix}$.
If $\theta$ is not a multiple of $\pi$, there are no real eigenvalues, because no vector stays on the same line (span) after rotation.

\subsubsection{Orthogonal Projection}
Let $P_{S}$ be an orthogonal projection onto subspace $S$.
\begin{itemize}
    \item If $x \in S \setminus \{0\}$, $x$ is an eigenvector with $\lambda=1$ (since $P_S(x)=x$).
    \item If $x \perp S$, $x$ is an eigenvector with $\lambda=0$ (since $P_S(x)=0$).
\end{itemize}