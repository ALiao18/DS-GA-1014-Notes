**motivation**
In machine learning, we often have to minimize functions:
$$
\begin{align*}
f(\theta)=Loss(data, model_{\theta}) \text{ with respect to } \theta\in \mathbb{R}^{n} \\
\end{align*}
$$
For $n=1,2$, we could plot $f$ to find the minimizer. However, this doesn't work for higher dimensions! Thus we can only conduct local searches by looking at the derivatives (gradient) in order to describe local behavior. Local information gives us global information when the function is *convex* . 
# table of contents
1. functions of multiple variables
2. convexity definitions
3. convexity and derivatives
4. Jensen's inequality
---
# functions of $n$ variables

## $n=1$: Single Variable

$$
\begin{align*}
f: \mathbb{R} &\rightarrow \mathbb{R} \\
x &\mapsto f(x)
\end{align*}
$$

We can visualize functions with 1 variable by plotting it out on a 2D graph. We can find $f'(x)$ by finding the tangent at $f(x)$ using point-slope form: $y-y_{0}=m(x-x_{0})$.

**First-Order Taylor Approximation:** Let $x=x_{0}+h$, then
$$f(x_{0}+h) - f(x_{0}) \simeq h \cdot f'(x_{0})$$
$$f(x_{0}+h)=f(x_{0})+h\cdot f'(x_{0}) + o(h)$$
where $o(h)$ represents some error term that becomes negligible compared to $h$. Another way to write it is $\epsilon(h)$ s.t. $\lim_{h \to 0}\frac{\epsilon(h)}{h}=0$, meaning the error shrinks faster than $h$.
## $n=2$: Two Variables

$$
\begin{align*}
f: \mathbb{R}^2 &\rightarrow \mathbb{R} \\
x &\mapsto f(x) =f(x_{1}, x_{2})
\end{align*}
$$

$n=2$ is the limit of our simple visualization, and we can draw a 3D graph of the gradient. The idea is to break it down into functions of single variables by looking at $h$ in each axis, where $h\in \mathbb{R}^2$.

1. Move along $x_{1}$: $f(x)\rightarrow f(x_{1}+h_{1},x_{2})$
2. Move along $x_{2}$: $f(x_{1}+h_{1},x_{2}) \rightarrow f(x_{1}+h_{1},x_{2}+h_{2})$

$$
\begin{align*}
f(x+h) - f(x) &= [f(x_{1}+h_{1},x_{2})-f(x_{1},x_{2})] + [f(x_{1}+h_{1},x_{2}+h_{2})-f(x_{1}+h_{1},x_{2})] \\
f(x+h) &=f(x)+ [f(x_{1}+h_{1},x_{2})-f(x_{1},x_{2})] + [f(x_{1}+h_{1},x_{2}+h_{2})-f(x_{1}+h_{1},x_{2})] \\
&\simeq f(x)+\left( \frac{\partial{f}}{\partial{x_{1}}}(x_{1},x_{2})h_{1} \right)+\left( \frac{\partial{f}}{\partial{x_{2}}}(x_{1}+h_{1},x_{2})h_{2} \right) \\
&\approx f(x)+\left( \frac{\partial{f}}{\partial{x_{1}}}(x_{1},x_{2})h_{1} \right)+\left( \frac{\partial{f}}{\partial{x_{2}}}(x_{1},x_{2})h_{2} \right)
\end{align*}
$$
We can approximate $\frac{\partial{f}}{\partial{x_{2}}}(x_{1},x_{2}) \approx \frac{\partial{f}}{\partial{x_{2}}}(x_{1}+h_{1},x_{2})$ because: (1) when $h_{1}$ is small, $(x_{1}+h_{1},x_{2})$ is very similar to $(x_{1},x_{2})$, (2) $\frac{\partial{f}}{\partial{x_{2}}}$ is continuous and we can assume local smoothness, (3) the error from this approximation is $O(h_{1}h_{2})$, second-order small.
## General Case: $n$ Variables

$$
\begin{align*}
f: \mathbb{R}^n &\rightarrow \mathbb{R} \\
x &\mapsto f(x) =f(x_{1}, \dots,x_{n})
\end{align*}
$$
We are no longer able to do a simple visualization, but we now know that we can break steps down into individual axes. We define the gradient:
$$\nabla f(x)=\left( \frac{\partial{f}}{\partial{x_{1}}}(x), \dots, \frac{\partial{f}}{\partial{x_{n}}}(x)\right)$$
Thus, 
$$
\begin{align*}
f(x)+\left( \frac{\partial{f}}{\partial{x_{1}}}(x_{1},x_{2})h_{1} \right)+\left( \frac{\partial{f}}{\partial{x_{2}}}(x_{1},x_{2})h_{2} \right) = f(x) +\langle \nabla f(x),h  \rangle
\end{align*}
$$
### Jacobian matrix
The Jacobian generalizes the gradient to **vector-valued functions** (functions with multiple outputs).

For a function with multiple outputs, we need to compute the gradient of each component function separately:
$$
\begin{align*}
f: \mathbb{R}^{n} &\rightarrow \mathbb{R}^m \\
   x &\mapsto f(x)=\begin{pmatrix}
f_{1}(x) \\
\vdots \\
f_{m}(x) 
\end{pmatrix} \\ \\
\mathbf{J}f: \mathbb{R}^{n} &\rightarrow \mathbb{R}^{m \times n} \\
x &\mapsto \mathbf{J}(x) = \begin{pmatrix}
\nabla f_{1}(x) \\
\vdots \\
\nabla f_{m}(x)
\end{pmatrix}
\end{align*}
$$

The Jacobian is an $m \times n$ matrix where each row is the gradient of one component function.
### Hessian matrix
The Hessian is the **second derivative of a scalar-valued function** (a function with a single real output).

While the Jacobian answers "what is the first derivative of a vector-valued function?", the Hessian answers "what is the second derivative of a scalar-valued function?" This is why we restrict to $f: \mathbb{R}^n \rightarrow \mathbb{R}$.

Starting with a scalar function and its gradient:
$$
\begin{align*}
f: \mathbb{R}^n &\rightarrow \mathbb{R} \\
x &\mapsto f(x) \\ \\
\nabla f: \mathbb{R}^n &\rightarrow \mathbb{R}^n \\
x &\mapsto \nabla f(x) = \left( \frac{\partial{f}}{\partial{x_{1}}}(x), \dots, \frac{\partial{f}}{\partial{x_{n}}}(x) \right)
\end{align*}
$$

The Hessian is obtained by taking the gradient of the gradient:
$$
\begin{align*}
\nabla^2 f: \mathbb{R}^n &\rightarrow \mathbb{R}^{n \times n} \\
x &\mapsto H(x) = \nabla^2f(x)
\end{align*}
$$

where 
$$H(x) = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{pmatrix}\in \mathbb{R}^{n \times n}$$

In general the order of differentiation of $x$ does not matter (Schwarz's Theorem). Thus the Hessian is a symmetric matrix. 

The Hessian is an $n \times n$ symmetric matrix containing all second partial derivatives. For cases of $\mathbb{R}^n \mapsto \mathbb{R}^m$, there would be $m$ Hessians, and can be represented by a $n \times n \times m$ tensor.  
#### Schwarz's Theorem
>Theorem
>If $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is twice differentiable*, then $\forall x\in \mathbb{R}, \forall i,j\in \{ 1,\dots,n \}$ we have:
>$\frac{\partial{}}{\partial{x_{i}}}\left( \frac{\partial{f}}{\partial{x_{j}}} \right)(x)=\frac{\partial{}}{\partial{x_{j}}}\left( \frac{\partial{f}}{\partial{x_{i}}} \right)(x)$

\*:= twice differentiable and second partial derivative must be continuous

The intuition is that moving $h_{1}$ in x axis and then $h_{2}$ in y axis is equivalent to moving $h_{2}$ in y axis and then moving $h_{1}$ in x axis. 

## Taylor's formula

We can now add one more term in Taylor's formula and get second order precision.
Let $x\in \mathbb{R}^\mathbf{n}$ As $h\in \mathbb{R}^n$ tends to zero, we have:
$$
\begin{align*}
f(x+h)=f(x)+\langle \nabla f(x),h  \rangle + \frac{1}{2}h^{\top}\nabla^{2}f(x)h+o(\lVert h \rVert )^{2}
\end{align*}
$$
$1d: f(x+hf(x)+f'(x)h+\frac{1}{2}f''(x)h^{2}+o(h^{2})$
# convexity definitions

## convex sets
A set $S \subset \mathbb{R}^n$ is called a convex set if $\forall x,y\in S, \alpha\in [0,1]$, $\alpha x + (1-\alpha)y\in S$.

We can think of a linear combination where the coefficients are non-negative and must sum to 1. So drawing 2 points in the plot, the shortest path between them must exist in the set entirely as well. 
### examples
1. Any subspace $S$ of $\mathbb{R}^n$ is convex
	1. closed under linear combinations
2. For any norm $\lVert \cdot \rVert$ and $r \geq 0$, the ball of radius $r: B(r)=\{ x\in \mathbb{R}^n | \lVert x \rVert \leq r\}$ is convex
	1. closed under linear combinations
## convex/concave functions
A function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex if $\forall x,y\in \mathbb{R}^n, \forall \alpha\in [0,1], f(\alpha x+(1-\alpha)y)\leq \alpha f(x)+(1-\alpha)f(y)$

The intuition is similar to convex sets except in this case its any 2 points on or "above" (enclosed by the function) must have its shortest path completely above the function as well. AKA its something that always curves upwards. 

An equivalent definition is that the epigraph is a convex set. 

A function $f$ is concave if $-f$ is convex
### properties/exercises
1. Show that any linear map $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is convex and concave
	1. $f(\alpha x+(1-\alpha)y)=,\leq,\geq \alpha f(x)=(1-\alpha)f(y)$. Since the path is completely on the function, it is both below and above, thus both convex and concave. 
2. Show that a norm $\lVert \cdot \rVert$ is convex
	1. $\lVert \alpha x + (1-\alpha)y \rVert \leq \alpha \lVert x \rVert + (1-\alpha) \lVert y \rVert$ (triangle inequality)
3. Show that the sum of two convex functions is also a convex function
	1. $x \mapsto \exp(x)$ is convex
# convexity and derivatives
*proposition:* convex functions and their tangents
A differentiable function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is convex $\iff \forall x,y\in \mathbb{R}^n$ $f(y)\geq f(x)+\langle \nabla f(x),(y-x)  \rangle$. This means the actual function is always above its tangent line (approximation).

*corollary:* 
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a differentiable convex function and $x\in \mathbb{R}^n$. Then 
$$
\begin{align*}
\text{x is a global minimizer of }f \iff \nabla f(x)=0
\end{align*}
$$
*proposition:* Hessian of a convex function
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a twice-differentiable function. Then $f$ is convex $\iff \forall x\in \mathbb{R}^n, \nabla^{2}f(x)$ is Positive semi-definite
1. in dimension $n=1$, a function $f$ is convex $\iff$ $f''(x) \geq 0$
2. Positive definite Hessian
	1. strictly convex function $h^{\top}\nabla^{2}f(x)h>0$ if $h \neq 0,\lambda h(\nabla^{2}f(x)>0)$
# Jensen's inequality
*Theorem:*
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function. Then for all $x_{1},\dots,x_{k}\in \mathbb{R}^n$ and all $\alpha_{1},\dots,\alpha_{k} \geq 0 \subset\to \sum_{i=1}^{k}\alpha_{i}=1$ we have 
$$
\begin{align*}
f\left( \sum_{i=1}^{k}\alpha_{i}x_{i} \right)\leq\sum_{i=1}^{k}\alpha_{i}f(x_{i})
\end{align*}
$$
More generally, if $X$ is a random variable that takes value in $\mathbb{R}^n$ we have
$$
\begin{align*}
f(\mathbb{E}[X])\leq\mathbb{E}[f(X)]
\end{align*}
$$
One result of Jensen's inequality is that the variance is always non-negative. 

*example: entropy*
Consider a random variable $X$ that takes values in $\{ 1,\dots,k \}$
Denote by $p_{i}$, the probability $P(X=i)$ for $i\in \{ 1,\dots,k \}$
The entropy of $X$ is defined by: $H(X)=\sum_{i=1}^{k}p_{i}\log\left( \frac{1}{p_{i}}\right)$
Observe that the logarithm is a concave function, so we'll have Jensen's inequality in the opposite direction!
$$
\begin{align*}
H(X)&=\sum_{i=1}^{k}p_{i}\log\left( \frac{1}{p_{i}} \right) \leq \log\left( \sum_{i=1}^{k}p_{i}\frac{1}{p_{i}} \right) \\
&= \log(k)
\end{align*}
$$
We observe that this is the entropy of the uniform distribution:
$H(X)=\sum_{i=1}^{k} \frac{1}{k} \log(k)=\log(k)$