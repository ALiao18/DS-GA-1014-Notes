# table of contents
1. spectral theorem
2. PCA
---
# spectral theorem
>Let $A \in \mathbb{R}^{n \times n}$ be a **symmetric** matrix. Then there is a **orthonormal basis** of $\mathbb{R}^{n}$ composed of eigenvectors of $A$. 

That means that if $A$ is symmetric, then there exists an orthonormal basis $(v_{1},\dots,v_{n})$ of $\mathbb{R}^{n}$ and $\lambda_{1},\dots, \lambda_{n} \in \mathbb{R} \text{ s.t. }$
$$
\begin{align*}
Av_{i}&=\lambda_{i}v_{i} \quad \forall i \in \{ 1,\dots,n \} \\
AV&=V\Lambda \tag{$\Lambda=diag(\lambda_{1},\dots,\lambda_{n})$} \\
A&=V\Lambda V^{-1} \tag{$V^{\top}V=Id, \therefore V^{-1}=V^{\top}$} \\
A&=V\Lambda V^{\top} \\
A&=\sum_{i=1}^{n}\lambda_{i}v_{i}v_{i}^{\top} \tag{$\lambda$ becomes scalar}
\end{align*}
$$
Geometrically, $\lambda_{i}v_{i}v_{i}^{\top}$ is the orthogonal projection of $\lambda_{i}$ onto $v_{i}$. 

>Theorem (Matrix formulation)
>Let $A \in \mathbb{R}^{n \times n}$ be a **symmetric** matrix. Then there exists an orthogonal matrix $P$ and a diagonal matrix $D$ of sizes $n \times n \text{ s.t. } A=PDP^{\top}$.
## the spectral orthonormal basis
Why are eigenvectors orthogonal when $A$ is symmetric?
Note that orthogonality $\subset$ linear independence

Let $v,w$ eigenvectors of $A$ for eigenvalue $\lambda \neq \mu$. 
$$
\begin{align*}
v^{\top}Aw &= v^{\top}(Aw)=v^{\top}(\mu w)=\mu v^{\top} w \\
           &= v^{\top}A^{\top}w = (Av)^{\top}w=\lambda v^{\top}w \\
           &\therefore (\mu-\lambda)v^{\top}w = 0 \\
           &\text{Since we know $\mu\neq\lambda, v^{\top}w$ must be 0} \\
           &\therefore\text{v and w are orthogonal}
\end{align*}
$$
Note that $A$ must be symmetric. 
## geometric interpretation
Note that orthogonality is a stronger form of independence. 

**tl;dr:** $Ax$ is equivalent to the orthogonal projection of $x$ onto the eigenvector (orthonormal basis), scaled by its corresponding eigenvalue ($\lambda$)

$Av_{i}=\lambda_{i}v_{i}$
$$
\begin{align*}
\text{Let } x \in \mathbb{R}^{n} \\
Ax &= A\left( \sum_{i=1}^{n} \langle x, v_{i} \rangle v_{i}\right) \\
   &= \sum_{i=1}^{n}\langle x_{i}, v_{i} \rangle Av_{i} \\
   &= \sum_{i=1}^{n} \lambda_{i} \langle x,v_{i}  \rangle v_{i} \\
   \langle Ax,v_{i}  \rangle &= \lambda_{i} \langle x,v_{i}  \rangle
\end{align*}
$$

To go from coordinates to vectors, take the linear combination. We can also look at it from another perspective:
$$
\begin{align*}
Ax&=\sum_{i=1}^{n} \lambda_{i}\langle x,v_{i}  \rangle v_{i} \\
  &=\sum_{i=1}^{n} \lambda_{i}v_{i}(v_{i}^{\top}x) \tag{$v_{i}^{\top}x$ gives a scalar} \\
  &=\sum_{i=1}^{n} \lambda_{i}(v_{i}v_{i}^{\top})x \tag{$v_{i}v_{i}^{\top}$ gives$n \times n$ matrix} \\
  &=(\sum_{i=1}^{n} \lambda_{i}v_{i}v_{i}^{\top})x \\ \\
A &= \sum_{i=1}^{n} \lambda_{i}v_{i}v_{i}^{\top}
\end{align*}
$$

we first have $x$, then
1. decompose into eigenvector basis $(\langle x,v_{i}  \rangle)_{i=n}^n$
2. scale by eigenvalues $(\lambda_{i}\langle x,v_{i}  \rangle)_{i=1}^n$
3. transform back to standard basis $\left( \sum_{i=1}^{n}\lambda_{i}\langle x_{i},v_{i}  \rangle v_{i}\right)$

Each of these operations can be encoded as a single matrix, where the decomposing is an orthogonal matrix of row vectors of $v_{i}^{\top}$ ($V^{\top}$), the scaling is a diagonal matrix, and the transformation back is $V$. Note that operations are right to left. 

$\therefore A=V\Lambda V^{\top}$

Zooming out, we know that orthogonal matrices (rotations) do not have eigenvectors (since its a rotation and no scaling!) and there is no basis where a rotation acts by scaling coordinates. The spectral theorem tells us that symmetric matrices always behaves as scaling operations in some orthonormal basis!
## consequences
If $A=PDP^{\top}$ for some orthogonal matrix $P$, then

1. $(\lambda_{1},\dots,\lambda_{n})$ are the only eigenvalues of $A$, and the number of times that an eigenvalue appears on the diagonal equals its multiplicity. 
2. the rank of $A$ equals the number of non-zero $\lambda_{i}$'s on the diagonal. $rank(A)=\#\{ i|\lambda_{i} \neq 0 \}$
3. $A$ is invertible if and only if $\lambda_{i}\neq 0$ for all $i$. In such case, $A^{-1}=P\left( \frac{1}{\lambda_{i}} \text{for all} \right)P^{\top}$
4. $Tr(A)=Tr(PDP^{\top})=Tr(DP^{\top}P)=Tr(D)=\lambda_{1}+\dots+\lambda_{n}$
	1. Note $Tr(BC)=Tr(CB)$

## proof sketch
Let $A$ be a $n \times n$ symmetric matrix and let $lamdba_{1}\geq \dots \geq \lambda_{n}$ be its $n$ eigenvalues and $v_{1},\dots,v_{n}$ be an associated orthonormal family of eigenvectors. Then
$$
\begin{align*}
\lambda_{1}=\max_{\lVert v \rVert = 1} v^{\top}Ax \quad\quad v_{1}=\arg\max_{\lVert v \rVert =1}v^{\top}Av
\end{align*}
$$
Moreover, for $k=2,\dots,n$:
$$
\begin{align*}
\lambda_{k}=\max_{\lVert v \rVert =1,v \perp v_{1}, \dots, v_{k-1}} v^{\top}Av \quad \quad v_{k}=\arg\max_{\lVert v \rVert =1,v \perp v_{1}, \dots, v_{k-1}}v^{\top}Av
\end{align*}
$$
# PCA
## empirical mean and covariance
Consider a dataset of $n$ points $a_{1},\dots,a_{n} \in \mathbb{R}^{d}$
### d=1
$\mu=\frac{1}{n}\sum_{i=1}^{n}a_{i} \in \mathbb{R}$
$\sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}(a_{i}-\mu)^{2} \in \mathbb{R}$
### d>1
now $a_{i}$ are vectors and its not clear how we take the square of a vector. 
Suppose we are in 2d, and we project all the points onto some axis
$a_{i}\rightarrow \langle a_{i},u  \rangle$
$$
\begin{align*}
\mu_{u}&=\frac{1}{n}\sum_{i=1}^{n}\langle a_{i},u  \rangle \\
       &= \langle \frac{1}{n}\sum_{i=1}^{n}a_{i},u  \rangle
\end{align*}
$$
$$
\begin{align*}
\sigma^{2}&=\frac{1}{n}\sum_{i=1}^{n}(\langle a_{i}-\mu,u  \rangle)^{2} \\
          &=\frac{1}{n}\sum_{i=1}^{n}u^{\top}(a_{i}-\mu)(a_{i}-\mu)^{\top}u \\
          &=u^{\top}\left( \frac{1}{n}\sum_{i=1}^{n}(a_{i}-\mu) (a_{i}-\mu)^{\top}\right)u \\
&\text{we call the matrix the covariance matrix } \mathbb{R}^{d \times d} \\
&\text{it describes the variance in all possible unit directions}
\end{align*}
$$
## PCA
Consider a dataset of $n$ points $a_{1},\dots, a_{n} \in \mathbb{R}^{d}$ where $d$ is somewhat large
Goal: find a lower dimensional representation, i.e. find $\tilde{a}_{1},\dots\tilde{a}_{n} \in \mathbb{R}^{k} \text{ s.t. } k \ll d$

We usually assume the data is centered: $\sum_{i=1}^{n}a_{i}=0$
Then, $S=\sum_{i=1}^{n}a_{i}a_{i}^{\top}=A^{\top}A$. It is symmetric. Thus we can apply the spectral theorem!
## direction of maximal variance
We aim to find $u$ where $\sigma^{2}_{u}$ is maximal 
So we solve $\max_{\lVert u \rVert=1}u^{\top}\Sigma u$ where $v$ is the eigenvector associated with the largest eigenvalue
$\lambda_{i}=v_{i}^{\top}\Sigma v_{i}=\sigma_{v_{i}}^{2}=\frac{1}{n}\sum_{i=1}^{n}\langle a_{i},v_{i}  \rangle^{2}\geq 0$
Project the $a_{i}$'s on $v_{1}$ $(\langle a_{i},v_{1}  \rangle)_{i=1:n}$

## j-th direction of maximal variance
The j-th direction of maximal variance is $v_{j}$ since $v_{j}$ is the solution of maximizing $v^{\top}Sv$ subject to $\lVert v \rVert = 1,v \perp v_{1}, v \perp v_{2}, \dots, v \perp v_{j-1}$

The dimensionally reduced dataset of in $k$ dimensions is then 
$\begin{pmatrix} \langle v_{1},a_{1}  \rangle \\ \langle v_{2},a_{1}  \rangle \\ \vdots \\ \langle v_{k},a_{1}  \rangle \end{pmatrix}, \begin{pmatrix} \langle v_{1},a_{2}  \rangle \\ \langle v_{2},a_{2}  \rangle \\ \vdots \\ \langle v_{k},a_{2}  \rangle \end{pmatrix}, \dots, \begin{pmatrix} \langle v_{1},a_{n}  \rangle \\ \langle v_{2},a_{n}  \rangle \\ \vdots \\ \langle v_{k},a_{n}  \rangle \end{pmatrix}$

## what value of $k$ should we take?
There is no mathematical theorem here, but common methods
1. elbow rule: eyeballing where there is a sharp decrease in additional variance explained
2. x%: however many principal components until it explains x% of variance


