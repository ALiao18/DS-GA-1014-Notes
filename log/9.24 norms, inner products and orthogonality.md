all about lengths and angles
# table of contents
1. norms
2. inner products
3. orthogonality

---
# norms
## euclidean norm
We define the Euclidean norm of $x=(x_{1},x_{2},\dots, x_{n}) \in \mathbb{R}^{n}$ as 
$$
\begin{align}
\lvert\lvert x \lvert\rvert_{2}=\sqrt{ x_{1}^{2}+\dots+x_{n}^{2} } 
\end{align}
$$
## general norms
For it to be a measure of length, it must satisfy 3 qualities:

Let $V$ be a vector space,
*definition:*
A norm $\lvert \lvert * \rvert \rvert$ on $V$ is a function from $V$ to $\mathbb{R}_{\geq 0}$ that verifies: 
1. *homogeneity:* $\lvert \lvert \alpha v \rvert \rvert = \lvert \alpha \rvert \times \lvert \lvert v \rvert \rvert \quad \forall\alpha \in \mathbb{R}, v \in V$.
2. *positive definiteness:* if $\lvert \lvert x \rvert \rvert=0$ for some $x \in V$, then $x=0$
3. *triangular inequality:* $\lvert \lvert u+v \rvert \rvert \leq \lvert \lvert u \rvert \rvert + \lvert \lvert v \rvert \rvert \quad \forall u, v \in V$

Other norms:
1. $l_1$ norm: aka taxi cab norm aka manhattan distance (absolute value satisfies triangular inequality)
	1. $\lvert \lvert x \rvert \rvert_{1}=\sum_{i=1}^{n}\lvert x_{i} \rvert$
2. $l_{\infty}$ norm
	1. $\lvert \lvert x \rvert \rvert_{\infty}=max(\lvert x_{1}\rvert, \dots, \lvert x_{n} \rvert)$
3. $l_{p}$ norm: converges to infinity norm
	1. $\lVert x \rVert_{p}=\left(\sum_{i=1}^{n} \lvert x_{i} \rvert^p\right)^{\frac{1}{p}}$

## exercise:
For each of the norms $\lVert \cdot \rVert_1, \lVert \cdot \rVert_2 , \lVert \cdot \rVert_{\infty}$, draw the "sphere":
$S=\{ x \in \mathbb{R}^{2} | \lVert x \rVert =1\}$, or the set of vectors such that the norm is 1. Note that a sphere is defined as having all the points being equidistance to the center. 

On $\lVert \cdot \rVert_1$, the 4 vectors are $(0, 1), (1, 0), (0, -1), (-1, 0)$. Connecting these 4 vectors forms a square. We can formalize this as:
$$
\begin{align}
\lVert x \rVert_1 &= \lvert x_{1} \rvert +\lvert x_{2} \rvert=1 \\
x_{1},x_{2}\geq 0&, \therefore x_{1}+x_{2}=1
\end{align}
$$
On $\lVert \cdot \rVert_2$, its the entire circle, one that includes the entire $\lVert \cdot \rVert_1$. We can formalize this as:
$$
\begin{align}
\lVert x \rVert_2 &= \sqrt{ x^{2}_{1}+x^{2}_{2} }=1 \\
x_{2}^{2}&=1-x_{1} \\
x_{2}&=\pm\sqrt{ 1-x_{1}^{2} }
\end{align}
$$
We can recognize that this is just the equation for a circle. 
On $\lVert \cdot \rVert_{\infty}$, this looks like a square that envelops both $\lVert \cdot \rVert_1, \lVert \cdot \rVert_2$. We can formalize it as:
$$
\begin{align}
\lVert x \rVert_{\infty} =max(\lvert x_{1} \rvert, \lvert x_{2} \rvert  )=1
\end{align}
$$
## application to regularized linear regression

Suppose this is an under-constrained (over-parameterized) solution. Thus the solution occupies an affine space. 

$X\theta=y$ 
Solution: $\{ \theta|X\theta=y \}$

How do you pick which $\theta$ if there are infinite solution?
By Occam's razor, lets take the simplest one, or $min \lVert \theta \rVert$ among solutions.

To visualize this, lets suppose on a 2D graph there is a line that doesn't cross the origin. Starting from the origin, we gradually increase the norm until some point of the norm intersects with the solution set. That is then the solution with the minimum norm among the infinite set. From this we can see why using different norms can give us different solutions. 
# inner products (review)
We define the Euclidean dot product of two vectors $x$ and $y$ of $\mathbb{R}^{n}$ as:
$$
\begin{align}
x \cdot y &= \sum_{i=1}^{n}x_{i}y_{i} \\
          &= x_{1}y_{1}+\dots+x_{n}y_{n} \\
          &= \lVert x \rVert \lVert y \rVert cos\theta
\end{align}
$$
This tells you about projections of 1 vector onto a another vector. If they are aligned, then their inner product is large, if they are orthogonal, their inner product is 0. If they are anti-aligned, their inner product could be a large negative value. 

In linear algebra, it is easier to work with linear product than angles. 
Note: $cos\theta= \frac{x\cdot y}{\lVert x \rVert\lVert y \rVert}$ = projection on the x axis. 
## inner product definition
Let $V$ be a vector space.
An inner product on $V$ is a function $\langle \cdot, \cdot  \rangle$ from $V \times V$ to $\mathbb{R}$ that verifies the following points:
1. *symmetry:* $\langle u, v \rangle = \langle v, u \rangle \quad \forall u, v \in V$
2. *bilinearity:* $\langle u+v,w  \rangle=\langle u,w  \rangle+\langle v,w  \rangle$ and $\langle \alpha v, w \rangle=\alpha\langle v,w  \rangle \quad \forall u, v, w \in V$ and $\alpha \in \mathbb{R}$
3. *positive definiteness:* $\langle v,v  \rangle\geq 0$ with equality if and only i $v=0$

### example
If $V$ is the set of all random variables (on a probability space $\Omega$) that have a finite second moment, then
$$
\begin{align}
\langle X,Y  \rangle \stackrel{\text{def}}{=} \mathbb{E}[XY]
\end{align}
$$
is an inner product on $V$. 

$\langle X,X  \rangle=\mathbb{E}[X^{2}]\geq 0$
if $\mathbb{E}[X^{2}]=0$, then $X=0$ almost surely

## induced norm
Proposition:
If $\langle \cdot,\cdot  \rangle$ is an inner product on V then, 
$\lVert v \rVert \stackrel{\text{def}}{=} \sqrt{ \langle v,v  \rangle }$
is a norm on $V$. We say that the norm $\lVert \cdot \rVert$ is induced by the inner product $\langle \cdot,\cdot  \rangle$

AKA: if you start from a valid inner product you get a valid norm
$-\lVert \alpha v\rVert=\sqrt{ \langle \alpha v,\alpha v  \rangle } = \sqrt{ \alpha^{2}\langle v,v  \rangle=\lvert \alpha \rvert \lVert v \rVert }$
If $\lVert v \rVert=0$, then $v=0$: positive definiteness of inner product
Triangular inequality? From Cauchy-Schwarz

Ex. For Euclidean inner product, $\sqrt{ v \cdot v }=\sqrt{ \sum v_{i}^{2}}= \lVert v \rVert_2$
### example 
Consider again the set $V$ of all random variables (on a probability space $\Omega$) that have a finite second moment, with the inner product:
$\langle X,Y  \rangle \stackrel{\text{def}}{=} \mathbb{E}[XY]$
$\lVert X \rVert=\sqrt{ \langle X, X \rangle }=\sqrt{ \mathbb{E}[X^{2}] }$
(if x is zero-mean, this is the standard deviation!)

$\lVert X+Y \rVert\leq \lVert X \rVert+\lVert Y \rVert$
ie $\sqrt{ \mathbb{E}[X+Y]^{2} }\leq \sqrt{ \mathbb{E}[X^{2}] }+\sqrt{ \mathbb{E}[X^{2}] }$

## Cauchy-Schwarz inequality
Theorem:
If $\lVert \cdot \rVert$ be the norm induced by the inner product $\langle \cdot,\cdot  \rangle$ on the vector space $V$. Then $\forall x, y \in V:$
$$
\begin{align}
\lvert \langle x,y  \rangle \rvert \leq \lVert x \rVert \lVert y \rVert
\end{align}
$$
Moreover, there is equality if and only if $x$ and $y$ are linearly dependent, i.e. $x=\alpha y$ or $y=\alpha x$ for some $\alpha \in \mathbb{R}$

1. prove cauchy schwarz
2. prove norm induced by inner product satisfies triangular inequality

### example
For random variables, with the inner product $\langle X,Y  \rangle=\mathbb{E}[XY]:$

CS: $\lvert \langle X,Y  \rangle \rvert\leq \lVert X \rVert \lVert Y \rVert$
$\lvert \mathbb{E}[XY] \rvert\leq \sqrt{ \mathbb{E}[X^{2}]\mathbb{E}[Y^{2}] }$
$\frac{\left\lvert  \mathbb{E}[XY]  \right\rvert}{\sqrt{ \mathbb{E}[X^{2}]\mathbb{E}[Y^{2}]}} \leq 1$
# orthogonality
Let $V$ be a vector space, and $\langle \cdot,\cdot  \rangle$ be an inner product on $V$.
1. We say that vectors $x,y$ are orthogonal if $\langle x,y  \rangle=0$. We write then $x \perp y$
2. We say that a vector $x$ is orthogonal to a set of vectors $S$ if $x$ is orthogonal to all vectors in $S$. We write then $x \perp S$.

Note that by definition, 0 is orthogonal to everything. 
### exercise
if $x$ is orthogonal to $v_{1},\dots,v_{k}$, then $x$ is orthogonal to any linear combination of these vectors, i.e., $x \perp Span(v_{1},\dots, v_{k})$. 

$\left\langle  x, \sum_{i=1}^{k}\alpha_{i}v_{i}   \right\rangle=\sum_{i=1}^{k}\alpha_{i}\langle x, v_{i},  \rangle=0$

## orthogonal and orthonormal families
definition
We say that a family of vectors $(v_{1},\dots,v_{k})$ is:
1. *orthogonal* if the vectors $v_{1},\dots,v_{n}$ are pairwise orthogonal, i.e. $\langle v_{i},v_{j}  \rangle=0 \quad \forall i \neq j$
2. orthonormal if it is orthogonal and if all the $v_{i}$ have unit norm: $\lVert v_{1} \rVert=\dots=\lVert v_{k} \rVert=1$

### coordinates in an orthonormal basis (think it out)
Proposition:
A vector space of finite dimension admits an orthonormal basis

Proposition:
Assume that $dim(V)=n$ and let $(v_{1},\dots,v_{n})$ be an orthonormal basis of $V$. Then the coordinates of a vector $x \in V$ in the basis $(v_{1},\dots,v_{n})$ are $(\langle v_{1},x  \rangle, \dots, \langle v_{n},x  \rangle)$:
$x=\langle v_{1},x  \rangle v_{1}+\dots+\langle v_{n}, x \rangle v_{n}$

Note that as a result of this, inverses of orthogonal matrices is just the tranpose of itself. Try to think about why. 

### example
Let $x,y \in V$ with coordinates $x=(\alpha_{1},\dots,\alpha_{n})$ and $y=(\beta_{1},\dots,\beta_{n})$ in an orthonormal basis $(v_{1},\dots,v_{n})$

$\langle x,y  \rangle= \left\langle  \sum_{i=1}^{n} \alpha_{i}v_{i}, \sum_{j=1}^{n}\beta_{j}v_{j}  \right\rangle=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\beta_{j}\langle v_{i},v_{j}  \rangle=\sum_{i=1}^{n}\alpha_{i} \beta_{i}$
$\lVert x \rVert=\sqrt{ \langle x, x \rangle } = \sqrt{ \sum_{i=1}^{n} \alpha_{i}^{2}}$

## applications

### pythagorean theorem
Let $\lVert \cdot \rVert$ be the norm induced by $\langle \cdot,\cdot  \rangle$. For all $x,y \in V$ we have:
$x \perp y \iff \lVert x+y \rVert^{2}=\lVert x \rVert^{2}+\lVert y \rVert^{2}$

Proof:
$$
\begin{align}
\lVert x+y \rVert^{2}&=\langle x+y, x+y \rangle \\
                     &=\langle x,x  \rangle+\langle y,x  \rangle+\langle x,y  \rangle + \langle y,y  \rangle \\
                     &= \lVert x \rVert^{2} + \lvert y \rvert^{2} + 2\langle x,y  \rangle   \\
                     &= 0 \iff x \perp y
\end{align}
$$
### random variables
For random variables with zero mean and finite second movement, with the inner product $\langle X,Y  \rangle=\mathbb{E}[XY]$

Pythagoreas: if $X \perp Y, \lVert X+Y \rVert^{2} = \lVert X \rVert^{2}+\lVert Y \rVert^{2}$
If $\mathbb{E}[XY]=0$, $\mathbb{E}[(X+Y)^{2}]=\mathbb{E}[X^{2}]+\mathbb{E}[Y^{2}]$
$Var(X+Y)=Var(X)+Var(Y)$

### picture
From now on, $\langle \cdot,\cdot  \rangle$ denotes the Euclidean dot product, and $\lVert \cdot \rVert$ the Euclidean norm. Let $x \in \mathbb{R}^{n}$ and $S$ a subspace of $\mathbb{R}^{n}$. What is the vector of $S$ that is the closest to $x$?

## orthogonal projection
Let $S$ be a subspace of $\mathbb{R}^{n}$. The **orthogonal projection** of a vector $x$ onto $S$ is defined as the vector $P_{S}(x)$ in $S$ that minimizes the distance to $x$:
$$
\begin{align}
P_{S(x)} \stackrel{\text{def}}{=} \underset{y \in S}{\operatorname{argmin}}\lVert x-y \rVert 
\end{align}
$$
It is the closest point according to some norm. 
### computing orthogonal projections
Proposition:
Let $S$ be a subspace of $\mathbb{R}^{n}$ and let $(v_{1},\dots,v_{k})$ be an orthonormal basis of $S$. Then for all $x \in \mathbb{R}^{n}$$$
\begin{align}
P_{S}(x)=\langle v_{1},x  \rangle v_{1}+\dots+\langle v_{k},x  \rangle v_{k}
\end{align}
$$
Proof:
Let $s \in S$, $s \langle v_{1},s  \rangle v_{1}+\dots+\langle v_{k},s  \rangle v_{k}=\alpha_{1}v_{1}+\dots+\alpha_{k}v_{k}$. Complete $v_{1},\dots,v_{k}$ into $v_{1},..,v_{m}$ an orthogonal basis of $\mathbb{R}^{2}$
$$
\begin{align}
\lVert x-s \rVert^{2} &= \left\lVert  \sum_{i=1}^{n} \langle x,v_{i}  \rangle v_{i} - \sum_{i=1}^{k} \alpha_{i}v_{i} \right\rVert^{2} \\
&= \left\lVert \sum_{i=1}^{k}(\langle x, v_{i} \rangle - \alpha_{i})v_{i} + \sum_{i=k+1}^{n} \langle x, v_{i} \rangle v_{i} \right\rVert^{2}  \\
&= \sum_{i=n}^{k}(\langle x,v_{i}  \rangle - a_{i})^{2} + \sum_{i=k:n}^{n} \langle x, v_{i} \rangle^{2}
\end{align}
$$
Since the second term has no $\alpha$, we are only concerned with the first term. We know that for any square term the minimum is 0, thus $\alpha_{i}=\langle x, v_{i} \rangle^{2}$. Thus, s defines $P_{s}$. 

### consequences
Let $V=\begin{pmatrix} | &  & | \\ v_{1} & \dots & v_{k} \\ | &  & | \end{pmatrix}$ gather the orthonormal basis vectors of the subspace $S$. 
Proposition:
The orthogonal projection is given by $P_{S}(x)=VV^{\top}x$
1. $P_{S}$ is a linear transform
2. $VV^{\top}$ is its matrix. 

**Think these through geometrically.**
$V^{\top}x$ matrix vector product is equivalent to taking inner products with the rows. 
$VV^{\top}x=\begin{pmatrix} | &  & | \\ v_{1} & \dots & v_{k} \\ | &  & | \end{pmatrix}\begin{pmatrix} . \\  \\  \end{pmatrix}$ each $.$ is the first coordinate of $(v_{1}^{\top}x)v_{1}+\dots+(v_{k}^{\top}x)v_{k}$, which is a linear combination of the $v_{i}$s, etc. Think of this geometrically. Taking the inner combination of the columns. 
The inner product $v^{\top}v$ gives you a scalar and the outer product gives you a matrix $vv^{\top}$.

### corollary
$\forall x \in \mathbb{R}^{n}$, 
1. $x-P_{S}(x)$ is orthogonal to $S$
2. $\lVert P_{S}(x) \rVert \leq \lVert x \rVert$

## orthogonal complement
Let $S$ be a subspace of $V$

We define the orthogonal complement of $S$ as
$$
\begin{align}
S^\perp = \{ x \in V | x \perp S\} \\
\end{align}
$$

Properties:
1. $S^\perp$ is a subspace of $V$
2. $dim(S^\perp) = dim(V)- dim(S)$ hint: think about proving it with kernel and image!