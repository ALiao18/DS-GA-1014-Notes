# table of contents:
1. review
2. linear transformations
3. matrices
4. kernel and Images
5. application to ML 
	1. Matrix multiplication of linear systems
	2. Gaussian elimination

---
# review
There are 2 interpretations of vectors: geometric and numerical. These two interpretations *bridge the abstract world and algorithmic world*. 
- The _geometric world_ provides the vectors, which contain scale and directionality
- The _numerical world_ provides the basis, which are the coordinates to project the vectors to.
	- let basis be $e_{1},e_{2},\dots,e_{n}$
	- $\vec{v} =v_{1}e_{1}+v_{2}e_{2}+\dots+v_{n}e_{n}$

In $\mathbb{R}^n$ with the [[canonical basis]], it is trivial because $\vec{v}=(v_{1},\dots,v_{n})=v_{1}\vec{e_{1}}+\dots+v_{n}\vec{e_{n}}$. 
However, when the vectors are functions, since they don't live in $\mathbb{R}^n$, they don't really have components and the $\mathbb{R}^n$ vector geometric interpretation is invalid. However, we can still describe talk about the coordinates of a function in a given basis. 

It is key to distinguish the two. 
# linear transformations
_geometric interpretation:_ operations done on vectors with linear properties
_numerical interpretation:_ arrays of numbers, aka matrices

Types of linear transformations:
For visualization look at [video1](https://www.youtube.com/watch?v=7Gtxd-ew4lk) , and [video2](https://www.youtube.com/watch?v=wciU07gPqUE). 

![[Screenshot 2025-09-13 at 10.10.46 PM.png]]
## Definition
Symmetries and rotations are linear mappings. So they are functions from vectors to vectors. 
$$\begin{aligned}
L:\quad\mathbb{R}^2 & \to\quad\mathbb{R}^2 \\
v & \mapsto\quad L(v),
\end{aligned}$$
A function $L: \mathbb{R}^m \to \mathbb{R}^n$ is linear $\iff$
1. Closed under addition: $\forall v, w \in \mathbb{R}^m, L(v+w)=L(v)+L(w)$
2. Closed under scalar multiplication: $\forall v \in \mathbb{R}^m, \alpha \in \mathbb{R}, L(\alpha v)=\alpha L(v)$

Note: 
1. They don't have to be in the same vector space!
2. They are transformations that comply with closed under linear combinations. 

### Examples:
#### $\mathbb{R}^2 \to \mathbb{R}^3$ linear transformation:
$$\begin{aligned}
L:\quad\mathbb{R}^2 & \to\quad\mathbb{R}^3 \\
(v_{1}, v_{2}) & \mapsto\quad (5v_{1}, 0, v_{1}+v_{2})
\end{aligned}$$
##### Proof:
Closure under addition: Let $(v_{1}, v_{2}), (w_{1},w_{2}) \in \mathbb{R}^2$
$$
\begin{align}
L(v+w)&=L((v_{1}+w_{1}), (v_{2}+w_{2})) \\
&=(5(v_{1}+w_{1}), 0, (v_{1}+w_{1})+(v_{2}+w_{2})) \\
&=(5v_{1},0,v_{1}+v_{2})+(5w_{1},0,w_{1}+w_{2}) \\
&=L(v)+L(w)
\end{align}
$$
Closure under scalar multiplication: Let $(v_{1},v_{2}) \in \mathbb{R}^2, \lambda \in \mathbb{R}$
$$
\begin{align}
L(\lambda v) &= L(\lambda v_{1}, \lambda v_{2}) \\
&= (5\lambda v_{1}, 0, \lambda(v_{1}+v_{2})) \\
&=\lambda(5v_{1}, 0, (v_{1}+v_{2}))  \\
&=\lambda L(v)
\end{align}
$$
$\therefore L$ is a linear transformation

#### $\mathbb{R} \to \mathbb{R}$ non-linear transformation:
$$
\begin{align}
L:\quad \mathbb{R} &\to \mathbb{R} \\
x &\mapsto\quad x^2
\end{align}
$$
##### Proof: 
Closure under addition: Let $(v_{1}), (w_{1}) \in \mathbb{R}$

$$
\begin{align}
L(v+w)&=(v_{1}+w_{1})^2 \\
&=v_{1}^2+w_{1}^2+2vw \\
L(v)+L(w)&=v_{1}^2+w_{1}^2 \\
\therefore L(v+w) \neq L(v)+L(w) \\
\therefore L \text{ is not a linear transformation}
\end{align}
$$

Or, using a counter example to demonstrate the square of the sums is not equal to the sum of the squares
$$
\begin{align}
L(1+2)&=(1+2)^2 \\
&=9 \\
L(1)+L(2)&=1+4 \\
&=5 \\
5 \neq 9 \\
\therefore L \text{ is not a linear transformation}
\end{align}
$$

## Properties
Let $L:\mathbb{R}^m \to \mathbb{R}^n$ is linear, then 
1. $L(0)=0$: 
	1. $L(0)=L(a-a)=L(a+(-a))=L(a)+L(-a)=L(a)-L(a)=0$
2. distributive over addition and scalar multiplication 
	1. $L\left( \sum_{i=1}^{k} \alpha_{i}v_{i} \right)=\sum_{i=1}^{k}\alpha_{i}L(v_{i}), \quad \forall \alpha_{i} \in \mathbb{R}, v_{i} \in \mathbb{R}^m$: 
	2. $$\begin{align}
	L\left( \sum_{i=1}^{k}\alpha_{i}v_{i} \right) &= L(\alpha_{1}v_{1}+\alpha_{2}v_{2}+\dots+\alpha_{k}v_{k}) \\
	&=L(\alpha_{1}v_{1})+L(\alpha_{2}v_{2}+\dots+\alpha_{k}v_{k}) \\
	&=L(\alpha_{1}v_{1})+L(\alpha_{2}v_{2})+\dots+L(\alpha_{k}v_{k}) \\
	&=\alpha_{1}L(v_{1})+\alpha_{2}L(v_{2})+\dots+\alpha_{k}L(v_{k}) \\
	&=\sum_{i=1}^{k}\alpha_{i}L(v_{i})
	\end{align}
	$$
3. Composition of linear maps is also linear: 
	1.  If $L:\mathbb{R}^{m}\to\mathbb{R}^{n}$ and $M:\mathbb{R}^{n}\to\mathbb{R}^{k}$ are both linear, then the composite function is also linear (Apply $L$ first then $M$) $$\begin{align} M \circ L: \mathbb{R}^{m} &\to \quad \mathbb{R}^{k} \\ v &\mapsto \quad M(L(v)) \end{align}$$
	2. Let $v_{1}, v_{2} \in \mathbb{R}^{m}$ $$\begin{align} M(L(v_{1}+v_{2}))&=M(L(v_{1})+L(v_{2})) \\ &=M(L(v_{1}))+M(L(v_{2})) \\ \therefore M\circ L \text{ is closed under addition} \end{align}$$
	3. Let $v \in \mathbb{R}^{m}, \lambda \in \mathbb{R}$ $$ \begin{align} M(L(\lambda v)) &= M(\lambda L(v)) \\ &=\lambda M(L(v)) \\ \therefore M\circ L \text{ is closed under scalar multiplication} \end{align} $$
	4. $\therefore \text{ composition of linear maps is also linear}$

# matrices

Let $L: \mathbb{R}^{m} \to \mathbb{R}^{n}$ be a linear transformation and $(e_{1},e_{2},\dots,e_{m})$ be the canonical basis of $\mathbb{R}^{m}$. 
Then, $$
\begin{align}
\forall x&=(x_{1},\dots,x_{m}) \in \mathbb{R}^{m} \\
x &= (x_{1}, 0,\dots,0)+(0, x_{2},\dots,0)+\dots+(0,\dots,0, x_{m}) \\
&= x_{1}e_{1}+x_{2}e_{2}+\dots+x_{m}e_{m} \\
\text{Then, } \\
L(x)&=L\left( \sum_{i=1}^{m}x_{i}e_{i} \right)=\sum_{i=1}^{m}x_{i}L(e_{i})
\end{align}
$$
This is property 2 of linear transformations in section: linear transformations. 
From the vectors $L(e_{1}),\dots,L(e_{m}) \in \mathbb{R}^{n}$, we can compute all $L(x) \quad \forall x \in \mathbb{R}^{m}$, meaning once we know what a linear transformation does to a basis, we know what it does to every possible vector. **very useful!** So, **a linear map is determined by its action on a basis**. This alludes to the distinction between the vectors ($x$) and the coordinates ($e$) the vector is projected onto. 

_why matrices are basically a change in basis:_ "First, decompose any input vector into any basis of the input space. Then, apply the linear map to the basis in the input space. You get vectors in the output space, where you decompose them into any basis of the output space. You end up with an array that tells you everything you need to know about the linear mapping"
## linear maps and matrices definitions
**matrix definition:**
An $n \times m$ matrix is an array with $n$ rows and $m$ columns. We denote by $\mathbb{R}^{n\times m}$ the set of all $n \times m$ matrices.

**Canonical Matrix of a linear map:**
We can encode a linear map $L: \mathbb{R}^{m} \to \mathbb{R}^{n}$ by a $n \times m$ matrix

We first choose a input basis and a output basis. The matrix is a representation of the linear transformation between the input and output bases. By default, we choose both bases as the canonical basis. Thus, we define the canonical matrix as the matrix used to represent the linear transformation between two canonical bases. 

For all dimensions, there are infinite ways to choose bases for both the input and output bases. Thus, the set of all input basis and output bases are both infinite. There is a 1-1 mapping between each input-output basis pair and each matrix representation of L. So, the set of all possible matrix representations of L is precisely as large as the set of all input output bases pairs. 

$\{ \text{input basis} \} \times \{ \text{output basis} \} \leftrightarrow \{ \text{All possible matrix representations of L} \}$. 

The canonical matrix of $L$ is the $n \times m$ matrix $\tilde{L}$ whose columns are $L(e_{1}),\dots,L(e_{m})$. 

$$ 
\begin{align}
\tilde{L} = \begin{pmatrix} | & | & \cdots & | \\
                               L(e_1) & L(e_2) & \cdots & L(e_m) \\
                               | & | & \cdots & |  \\
               \end{pmatrix} &= 
               \begin{pmatrix} L_{1,1} & L_{1,2} & \cdots & L_{1,m} \\
                               L_{2,1} & L_{2,2} & \cdots & L_{2,m} \\
                               \vdots & \vdots & \ddots & \vdots \\
                               L_{n,1} & L_{n,2} & \cdots & L_{n,m}  \\
              \end{pmatrix}  \\
\text{Where we write } L(e_{j}) &= \begin{pmatrix} L_{1,j} \\ L_{2,j} \\ \vdots \\ L_{n,j} \end{pmatrix}
\end{align}
$$
### examples
1. **identity matrix $I$:**
	The identity map is linear. What is the canonical matrix of $Id$? $$ \begin{align} Id: \mathbb{R}^{n} &\to \quad \mathbb{R}^{n} \\ x &\mapsto \quad x  \\ \\ \text{Let }L: \mathbb{R}^{2} &\to \quad \mathbb{R}^{3} \\ (v_{1},v_{2}) &\mapsto \quad (5v_{1},0,v_{1}+v_{2})\\ \\ \tilde{L} = \begin{pmatrix} | & | \\ L(1,0) & L(0, 1) \\ | & | \end{pmatrix} &= \begin{pmatrix} 5\times 1 & 5 \times 0 \\ 0 & 0 \\ 1+0 & 0+1 \end{pmatrix} \\ \\ \tilde{Id} = Id \end{align}$$
2. **Homothety (Scale):**
	Let $\lambda \in \mathbb{R}$. The homothety map of ratio $\lambda$ is linear. What is the canonical matrix of $H_{\lambda}$? $$ \begin{align} H_{\lambda}: \mathbb{R}^{n} &\to \quad \mathbb{R}^{n} \\ x &\mapsto \quad \lambda x \\ \\ H_{\lambda}(e_{1}) &= \lambda(e_{1}), H_{\lambda}e_{2} = \lambda e_{2},\dots,H_{\lambda}(e_{n})=\lambda e_{n} \\ \\ \tilde{H_{\lambda}} = \begin{bmatrix} \lambda & 0 & \dots & 0 \\ 0 & \lambda & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda  \end{bmatrix} &= \lambda I_{n} \end{align} $$
3. ðŸŒŸ**Rotations in $\mathbb{R}^{2}$:**
	Let $\theta \in \mathbb{R}$. The rotation $\mathbb{R}_{\theta}:\mathbb{R}^{2} \to \mathbb{R}^{2}$ of angle $\theta$ about the origin is linear. What is the canonical matrix of $\mathbb{R}_{\theta}$? $$ \begin{align} \mathbb{R}_{\theta}(e_{1})&=(cos \theta, sin \theta)  \\ \mathbb{R}_{\theta}(e_{2})&=(-sin \theta, cos \theta) \\ \tilde{\mathbb{R}_{\theta}}&=\begin{pmatrix} cos \theta & sin \theta \\ -sin \theta & cos \theta  \end{pmatrix} \end{align} $$
## matrix-vector product
Consider a linear map $L: \mathbb{R}^{m} \to \mathbb{R}^{n}$ and its associated matrix $\tilde{L}\in\mathbb{R}^{n\times m}$. 
Can we use the matrix $\tilde{L}$ to compute the image $L(x)$ of a vector $x \in\mathbb{R}^{m}$?
$$
\begin{align}
&\text{Definition of matrix vector product:}  \\
&i \in \{ 1,\dots,m \} \\
L(x)_{i}&=L\left( \sum_{j}^{}x_{j}e_{j} \right)_{i} \\
&=\left( \sum_{j}^{}x_{j}L(e_{j}) \right)_{i} \\
&=\sum_{j}^{}x_{j} L(e_{j})_{i} \\
&=\sum_{j}^{}\tilde{L}_{ij}x_{j}
\end{align}
$$
So, for all $x \in \mathbb{R}^{m}$ we have $L(x)=\tilde{L}x$ where the matrix vector product $\tilde{L}x \in \mathbb{R}^{n}$ is defined by:
$(\tilde{L}x)_{i}=\sum_{j=1}^{m}L_{i,j}x_{j}=L_{i,1}x_{1}+L_{i,2}x_{2}+\dots+L_{i,m}x_{m}  \quad \forall i \in \{ 1,\dots,n \}$. 

So, the each element in the output matrix corresponds to the weighted sum of a row in the input matrix and the vector such that the vector is the scaling factor.

Note that in the statement $L(x)=\tilde{L}x$, the x on the left represents the vector while the x on the right represents coordinates, since we default to the canonical basis. 

For example,
$L: \mathbb{R}^{2} \to \mathbb{R}^{2}$ such that $L(x_{1},x_{2})=(2x_{1}+3x_{2}, -x_{1}+4x_{2})$ is a linear map. Using the default canonical matrix, $\tilde{L}=\begin{pmatrix} 2 & 3 \\ -1 & 4 \end{pmatrix}$is the canonical matrix. Using $x=\begin{pmatrix} 1 \\ 2 \end{pmatrix}$ as an example, 
$$
\begin{align}
\tilde{L}x&=\begin{pmatrix} 2 & 3 \\ -1 & 4 \end{pmatrix}\cdot\begin{pmatrix} 1 \\ 2 \end{pmatrix} \\
&=\begin{pmatrix}
2\cdot 1 + 3 \cdot 2 \\
-1 \cdot 1 + 4 \cdot 2 
\end{pmatrix} \\
&=\begin{pmatrix}
8 \\
7 
\end{pmatrix}
\end{align}
$$
## addition and scalar multiplication
We know that linear maps are compatible with linear combinations. Meaning they are closed under addition and scalar multiplication. This means we can sum and scale matrices and keep everything consistent. 
Naturally the sum of two matrices should maintain dimensions. We can think of adding matrices as adding linear maps. 

Since matrices are basically linear maps, they are vector spaces as well. $\mathbb{R}^{n \times m}$ is a vector space
Since it is a vector space, we can get its dimension. $dim(\mathbb{R}^{n \times m})=n \times m$. 
## matrix product
Let $L: \mathbb{R}^{n} \to \mathbb{R}^{n}$ and $M: \mathbb{R}^{m} \to \mathbb{R}^{k}$ be two linear maps. What is the matrix of $M \circ L: \mathbb{R}^{n} \to \mathbb{R}^{k}$?
$$
\begin{align}
\tilde{M \circ L} = \tilde{M}\cdot \tilde{L} \tag{matrix-matrix prod.}\\

\end{align}
$$
The formula:
The matrix product $ML$ is the $k\times n$ matrix of the linear map $M \circ L$.
The inner dimension must align
Its coefficients are given by the formula: $$
\begin{align}
(ML)_{i,j}=\sum_{l=1}^{m}M_{i,l}L_{l,j}=M_{i,1}L_{1,j}+\dots+M_{i,m}L_{m,j} \\
\forall 1 \leq i \leq k, 1 \leq j \leq n
\end{align}
$$
### Properties:
1. $(A+B)C=AC+BC$ Proof: compute the ij entry of both the lhs and rhs
2. $A(C+D)=AC+AD$ Same proof
3. Multiplication by the identity: $AId_{m}=A$
4. Commutativity? Not in most cases! $AB \neq BA$  
5. Can we divide matrices? As in if we have $AB=AC$, do we have $B=C$?
	1. Not in most cases!
		1. if A = 0, doesn't work
		2. if A is non-zero, but not invertible, doesn't work
## Invertible matrices
A square matrix $M \in \mathbb{R}^{n \times n}$ is called _invertible_ if there exists a matrix $M^{-1}\in\mathbb{R}^{n \times n}$ such that:
$MM^{-1}=M^{-1}M=Id_{n}$
Such matrix $M^{-1}$ is _unique_ and called the inverse of $M$. 
### Property:
1. Let $A, B \in \mathbb{R}^{n \times n}$. If $AB=Id_{n}$, then $BA=Id_{n}$. 
To show that $A$ is invertible, it is sufficient to find a matrix $B$ such that $AB=Id_{n}$. 
2. If $AB=AC$ and $A$ is invertible, $A^{-1}AB=A^{-1}AC$, so $B=C$
# kernels and images

Let $L: \mathbb{R}^{m} \to \mathbb{R}^{n}$ be a linear transformation

_kernel definition:_
The kernel $Ker(L)$ or nullspace of $L$ is defined as the set of all vectors $v \in \mathbb{R}^{m}$ such that $L(v)=0$. The kernel is the set of all vectors that collapse to zero after the linear transformation. 
$Ker(L):=\{ v \in \mathbb{R}^{m} | L(v)=0\}$ 

$Ker(L)$ is a subspace of $\mathbb{R}^{m}$

We can also define a kernel for a matrix $M$
$Ker(M)=\{ x \in \mathbb{R}^{m}|Mx=0 \}$

_image definition:_
The image (aka range, aka column space) of $L$ is defined as the set of all vectors $v \in \mathbb{R}^{n}$ such that there exists $v \in \mathbb{R}^{m}$ such that $L(v)=v$. In other words, it is the space occupied by all possible output vectors.
$Im(L)=\{ L(v)|v \in \mathbb{R}^{m} \} \subseteq \mathbb{R}^{n}$

$\mathrm{Im}(L)$ is a subspace of $\mathbb{R}^{n}$
$\mathrm{Im}(L)$ is also the span of the columns of the matrix representation of L.

We can find the image of a matrix $M$ as well.
$\mathrm{Im}(M)=\{ y \in \mathbb{R}^{n}| \exists x \in \mathbb{R}^{m}: y = Mx\}$
# application to ML 

## matrix multiplication of linear systems
### linear systems:
_problem_
$\text{Given data of input-output pairs } \{ (x_{1},y_{1}),\dots,(x_{n},y_{n}) \}$
$\text{ s.t. inputs } x_{i}=(x_{i,1},\dots,x_{i,m}) \in \mathbb{R}^{m}, \text{ outputs } y_{i} \in \mathbb{R}$
$\text{Given a new x, how can we predict y?}$

_hypothesis_
$\exists (\theta_{1},\dots,\theta_{m}) \text{ s.t. } \theta_{1}x_{i,1}+\theta_{2}x_{i,2}+\dots+\theta_{m}x_{i,m}=y_{i}$

_new problem_
$\text{How do we find } \theta\text{ ? Do we need more data?}$
$\exists \theta \text{ s.t. } X\theta=y \text{ ? Yes} \iff y \in \mathrm{Im}(x)$.
If $y \not\in \mathrm{Im}(x): \text{no solution}$
If $y \in \mathrm{Im}(x): \text{at least 1 solution}$
If $\theta \text{ is a solution, } X\theta=y=\theta_{0} \therefore X\theta-X\theta_{0}=y-y=0 \therefore \theta - \theta_{0} \in Ker(X)$.
Solution $\{ \theta_{0}+v, v \in Ker(X) \}=\theta_{0}+Ker(X)$, Sum between the subspace and the vector
If $Ker(X)=\{ 0 \}: \text{ one unique solution } \theta_{0}$
If $Ker(X)\neq \{ 0 \}: \text{infinite solutions}$

So there are either 0, 1, or infinite solutions. 
### matrix notation of linear systems
$X = \begin{pmatrix} x_{1,1} & \dots & x_{1,m} \\ \vdots & \ddots & \vdots \\ x_{n,1} & \dots & x_{n,m} \end{pmatrix} \in \mathbb{R}^{n \times m}, y=\begin{pmatrix} y_{1} \\ \vdots \\ y_{n} \end{pmatrix} \in \mathbb{R}^{n}, \theta=\begin{pmatrix} \theta_{1} \\ \vdots \\ \theta_{m} \end{pmatrix} \in \mathbb{R}^{m}$
$y=X\theta$

## gaussian elimination

*Goal:* To get matrices into row echelon form 
*Rules:* You are allowed 3 operations: 1. row swap, 2. row scale, 3. row addition

