# table of contents
1. SVD
2. graphs and graph Laplacian
3. application: spectral clustering
---
# SVD
"this is the single most powerful matrix decomposition, and once you know it, well, there's a before and after." - Florentein Guth

Motivation:
How do we generalize the spectral theorem decomposition to generic matrices $A \in \mathbb{R}^{n\times m}$? 

Since $A$ is now rectangular, $Av_{i}=\lambda_{i}v_{i}$ no longer has any meaning since the inner spaces no longer match up between $A, v_{i}$. 

*idea:* $Au_{i}=\sigma_{i}v_{i}$ for another orthonormal family $v_{1},\dots,v_{m}$. 
So, we know $A$ is an operation from $\mathbb{R}^{m} \to \mathbb{R}^{n}$ such that $Av_{i}=\sigma_{i}u_{i}$. 
$$
\begin{align*}
Ax&=A\left( \sum_{i=1}^{m}\langle x,v_{i}  \rangle v_{i}\right) \\
  &=\sum_{i=1}^{m}\langle x,v_{i}  \rangle Av_{i} \\
  &=\sum_{i=1}^{m}\sigma_{i}\langle x,v_{i}  \rangle u_{i} \\
  &=\sum_{i=1}^{m}\sigma_{i}(v_{i}^{\top}x)u_{i} \\
  &=\left( \sum_{i=1}^{m}\sigma_{i}u_{i}v_{i}^{\top} \right) x \\
  &=Ax \\
A &= \sum_{i=1}^{m}\sigma_{i}u_{i}v_{i}^{\top}
\end{align*}
$$
we first have $x$, then
1. decompose into eigenvector basis, aka rotate: $(\langle x,v_{i}  \rangle)_{i=1:m}$
2. scale by eigenvalues $(\sigma_{i}\langle x,v_{i}  \rangle)_{i=1:m}$
3. transform output space $\left( \sum_{i=1}^{m}\sigma_{i}\langle x_{i},v_{i}  \rangle u_{i}\right)$

Any linear combination can take this form. 
Note that we do not call them eigenvalues because eigenvalues implies $u_{i}=v_{i}$. This is a different concept. 

>Theorem:
>Let $A\in \mathbb{R}^{n \times m}$. Then there exists two orthogonal matrices $U\in \mathbb{R}^{n \times n}$ and $V\in \mathbb{R}^{m \times m}$ and a matrix $\Sigma\in \mathbb{R}^{n \times m}$ such that $\Sigma_{1,1} \geq \Sigma_{2,2} \geq \dots \geq 0$ and $\Sigma_{i,j}=0$ for $i \neq j$ that verify $A=U\Sigma V^{\top}$Â 

Notice that $\Sigma$ is a rectangular matrix. We know since $A$ is a rectangular matrix, A isn't invertible, and thus has a non-trival kernel. Therefore $\exists x \neq 0 \text{ s.t. } Ax=0$. The zero columns correspond to directions in $Ker(A)$. Pair this with the rank-nullility theorem. 
## proof
We want to show that $Av_{i}=\sigma_{i}u_{i}$ for orthonormal bases $\{v_i\}$ and $\{u_i\}$.

1. Establish orthogonality of $Av_i$
Starting with the inner product of $Av_i$ and $Av_j$:
$$
\begin{align*}
\langle Av_{i},Av_{j}  \rangle &= \langle A^{\top}Av_{i},v_{j}  \rangle \\
&= \langle \sigma_{i}u_{i},\sigma_{j}u_{j}  \rangle \\
&= \sigma_{i}\sigma_{j} \langle u_{i},u_{j}  \rangle \\
&= \begin{cases}
\sigma_i^2 & \text{if } i = j \\
0 & \text{if } i \neq j
\end{cases}
\end{align*}
$$

This shows that $Av_i$ and $Av_j$ are orthogonal when $i \neq j$.

2. Show $v_i$ are eigenvectors of $A^{\top}A$

From Step 1, $A^{\top}Av_{1}$ is orthogonal to $v_{2},\dots,v_{m}$. Since $\{v_1, \ldots, v_m\}$ form an orthonormal basis, this means $A^{\top}Av_{1}$ must be parallel to $v_1$:
$$A^{\top}Av_{1}=\lambda_{1}v_{1}$$
Therefore $v_{1}$ is an eigenvector of $A^{\top}A$. The same argument applies to all $v_i$.

3. Apply spectral theorem

Since $A^{\top}A$ is symmetric, we can apply the spectral theorem to obtain a complete orthonormal basis $\{v_{1},\dots,v_{m}\}$ of $\mathbb{R}^{m}$ consisting of eigenvectors.

4. Define singular values and left singular vectors

For each $v_i$ where $Av_{i} \neq 0$, define:
$$u_{i}=\frac{Av_{i}}{\lVert Av_{i} \rVert} \quad \text{and} \quad \sigma_{i}=\lVert Av_{i} \rVert$$

This construction ensures that $\{u_i\}$ forms an orthonormal set (by Step 1) and $Av_i = \sigma_i u_i$ by definition.

## remarks
1. Right singular vectors $v_{i}$'s are eigenvectors of $A^{\top}A\in \mathbb{R}^{m\times m}$, with eigenvalues $\lambda_{i}=\sigma^{2}_{i}$
	1. $A=U\Sigma V^{\top}, A^{\top}A=V\Sigma^{\top}U^{\top}U\Sigma V^{\top}=V\Sigma^{\top}\Sigma V^{\top}$
2. Left singular vectors $u_{i}$'s are eigenvectors of $AA^{\top}\in \mathbb{R}^{n \times n}$, with eigenvalues $\lambda_{i}=\sigma_{i}^{2}$
	1. similar to above
3. if $A$ is symmetric, then $u_{i}=v_{i}$. 
## non-negative singular values and sign instability
Singular values $\sigma_{i}=\lVert Av_{i} \rVert$ are defined by norms and measure the scaling of the vector $v_{i}$. Thus they are non-negative. We can see from another perspective that $A^{\top}Av_{i}=\lambda_{i}v_{i}$ where $\lambda_{i} = \lVert Av_{i} \rVert^{2} \geq 0, \sigma_{i}=\sqrt{ \lambda_{i} } \geq 0$. 

Since $Av_{i}=\sigma_{i}u_{i}$ and $\sigma_{i}>0$, we have $u_{i}=\frac{Av_{i}}{\sigma_{i}}$. If we flip the sign of $v_{i}$, $A(-v_{i})=-Av_{i}=\sigma_{i}(-u_{i})$. Thus we see that both $(v_{i},u_{i})$ and $(-v_{i},-u_{i})$ are valid pairs and is unique to signs. Numerical algorithms can sometimes be sign unstable but the answers across runs are all equally valid. 

# graphs and graph laplacian
Note that we are assuming *undirected graphs* 
## definitions
Consider a graph $G$ made of $n$ nodes with some edges. Matrices $A, D, L$ described below are all $\mathbb{R}^{n \times n}$
The adjacency matrix $A$ of $G$ is the $n \times n$ matrix with entries $$
A=\begin{cases}
1 & \text{if edges between nodes i and j} \\ \\
0 & \text{otherwise}
\end{cases}
$$
The _degree matrix_ $D\in \mathbb{R}^{n\times n}$ of $G$ is the diagonal matrix with
$$
\begin{align*}
D_{i,i} = \#\{ \text{neighbors of } i \}=deg(i)
\end{align*}
$$

## graph Laplacian
The Laplacian matrix of $G$ is defined as $L=D-A$. It just so happens that defining $L$ like that results in 
$$
\forall x\in \mathbb{R}^{n}, x^{\top}Lx=\sum_{i \sim j}(x_{i}-x_{j})^{2}
$$
This can be considered a measure of smoothness w.r.t. the graph, smooth meaning *nodes that are linked by an edge are similar in values.* 

Proof:
$$
\begin{align*}
\sum_{i \sim j}(x_{i}-x_{j})^{2} &= \frac{1}{2}\sum_{i=1}^{n}\sum_{\text{j neigh. of i}}(x_{i}^{2}+x_{j}^{2}-2x_{i}x_{j}) \\
&= \frac{1}{2} \sum_{i=1}^{n}deg(i)x_{i}^{2}+ \frac{1}{2}\sum_{j=1}^{n}deg(j)x_{j}^{2} - \sum_{ij}a_{ij}x_{i}x_{j} \\
&=x^{\top}Dx - x^{\top}Ax \\
&=x^{\top}(D-A)x = x^{\top}Lx
\end{align*}
$$
## properties of the Laplacian
For all $x\in \mathbb{R}^{n}$, $x^{\top}Lx=\sum_{i\sim j}(x_{i}-x_{j})^{2}$

Symmetric, PSD, non-invertible
1. *symmetric*: $L$ is symmetric because $D$ and $A$ are symmetric (difference of symmetric matrices is symmetric)
2. *PSD:* For any $x\in \mathbb{R}^{n}$, $$\begin{align*}
	x^{\top}Lx &=\sum_{i \sim j}(x_{i}-x_{j})^{2} \geq 0 \\
	&\therefore \text{L is PSD} \\
	&\therefore \lambda_{i}\geq 0 \forall 1 \leq i \leq n
	\end{align*}$$
3. *non-invertible:* Reminder that the spectral theorem implies existence of $Lv_{i}=\lambda_{i}v_{i}$, where $v_{1},\dots,v_{n}$ forms the orthonormal basis and $\lambda_{1} \leq \lambda_{2} \leq \dots \leq \lambda_{n}$. 
	We can select $\lambda_{1} = v_{1}^{\top} L v_{1} = \displaystyle\min_{\|v\| = 1} \sum_{i \sim j} (v_i - v_j)^2$. Note that finding the minima means finding a vector $v$ that varies the least across connected nodes, by the smoothness interpretation, it is the smoothest function. Without constraints, a trivial solution would be $v=0$. To avoid this constraint, we impose $\lVert v \rVert=1$. 
	This is the Rayleigh quotient minimization problem. Thus we can always set $v_{1}=\frac{1}{\sqrt{ n }}(1,\dots,1), \lambda_{1}=0$. 
	$\therefore Ker(L)$ is not empty
	$\therefore L$ is non-invertible

To find the next eigenvector, we have to impose an additional constraint, that is $v_{2} \perp v_{1}$:
$$
\begin{align*}
v_{2}=\displaystyle\arg\min_{\lVert v \rVert  = 1; w^{\top}v_{1}=0}\sum_{i\sim j}(w_{i}-w_{j})^{2}
\end{align*}
$$
We can interpret this as the "second" smoothest function. It turns out that if you have a disconnected v=graph (components with no edges), a function that is constant within the component is also perfectly smooth. For example:
$$
v=\begin{cases}
+1 & \text{on component 1}\\
-1 & \text{on component 2}
\end{cases}
$$
Thus, $v_{2}$ could also have $\lambda_{2}=0$. 

## algebraic connectivity
The previous result can be generalized to 
1. the multiplicity of the eigenvalue 0 of $L$ is equal to the number of connected components of $G$
2. $G$ is connected $\iff$ $\lambda_{2} >0$

$\lambda_{2}$ is known as algebraic connectivity of $G$ and measures how well $G$ is connected. From now, we assume $G$ is connected, meaning $\lambda_{2}>0$.

# application: spectral graph clustering
## two clusters: minimal cut problem
*setup:* The cut of $S \subset \{ 1,2,\dots,n \}$, denoted $cut(S)$ is defined as the number of edges between $S$ and $S^C$. 

*Goal:* find 2 clusters ($S, S^C$) such they are balanced (same number of nodes) and minimize the edges across. 

First we encode the set $S$ by a sign vector $x\in \{ +1,-1 \}^n$:
$$
x_{i}=\begin{cases}
+1 & \text{if } i\in S \\
-1 & \text{if } i\not\in S
\end{cases}
$$
Recall that since $L=D-A$, $x^{\top}Lx=\sum_{i\sim j}(x_{i}-x_{j})^{2}$. 
Thus if $i=j, x_{i}=x_{j} \Rightarrow (x_{i}-x_{j})^{2}=0$. On the other hand, if $i \neq j, (x_{i}-x_{j})^{2}=4$. Since each crossing edge contributes 4, and edges on the same side contribute 0, we can find the define:
$$
\begin{align}
cut(S)&=\frac{1}{4}x^{\top}Lx 
\end{align}
$$
Moreover, minimizing the cut is equivalent to minimizing $x^{\top}Lx$ over sign vectors. 

*subgoal:* $\displaystyle\min_{x\in \{ -1,1 \}^n}x^{\top}Lx \text{ s.t. }x \perp 1$.
The constraint $x \perp 1$ forces balance in $x$, meaning as many $+1$ as $-1$. This is a search over discrete $\{ -1,1 \}^n$ is a combinatorial problem and is NP-hard.

*new subgoal:* $\displaystyle\min_{\lVert v \rVert^{2}=n}x^{\top}Lx \text{ s.t. } x \perp 1$
We relax the constraints to be $v_{i}\in \mathbb{R}$. Thus makes the optimization space a. "sphere" orthogonal to 1. Note that from the properties of the Laplacian section, we see that this is solving for the eigenvector with the second smallest eigenvalue, aka $v_{2}$. This goal thus has a closed form solution: $v=\sqrt{ n }v_{2}$. 

## spectral clustering: two clusters
1. compute $v_{2}$, the second eigenvector of $L$.
2. $x_{i}=v_{2}(i)$
3. $S=\{ i|v_{2}(i) \geq \delta\}, S^C=\{ i|v_{2}(9)<\delta \}$. Usually, $\delta=0$.
## spectral clustering: k clusters
Input: Graph Laplacian $L$, number of clusters $k$
1. compute $v_{1},\dots,v_{k}$ of $L$ (note $v_{1}=1, v_{1}(i)=1$)
2. $x_{i}=(v_{2}(i),\dots,v_{k}(i))$
3. cluster $x_{1},\dots,x_{n}$ with some clustering algorithm (e.g k-means)
4. deduce node clusters from the cluster labels in the embedding space
# supplemental resources
1. https://www.youtube.com/watch?v=uTUVhsxdGS8: Spectral Graph Theory for Dummies. 