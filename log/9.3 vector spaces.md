# table of contents
1. vector spaces
2. span and linear families
3. basis and dimension

---
# vectors
There are 2 different interpretations of vectors, governed by Mathematical rules. 
vectors are defined *mathematically* by general operations: to scale and to sum

*geometric Interpretation:* direction (arrows)
*numerical Interpretation:* an array of numbers
## Vector Space
There are 8 properties that make up a vector space: 
A vector space consists of a set $V$ containing vectors
	**additive:** sum of 2 vectors is a vector
		$\forall \vec{x}, \vec{y} \in V, \vec{x} + \vec{y} \in V$
	**scalable:** multiple of vector by a scalar is a vector
		$\forall \vec{x} \in V, \lambda \in \mathbb{R}, \lambda \cdot \vec{x} \in V$
	**Commutative & Associative:** vector sums can be swapped around and parenthesized without a difference
		$\forall \vec{x}, \vec{y}, \vec{z} \in V, \vec{x} + \vec{y} = \vec{y} + \vec{x} \text{ and } \vec{x}+(\vec{y}+\vec{z})=(\vec{x} + \vec{y})+\vec{z}$
	**Contains $\vec{0} \in V$:** contains zero vector
		 $\forall \vec{x}, \vec{x}+\vec{0}=\vec{x}$
	**Additive Inverse:** Each vector has a negative vector that when added, becomes 0
		For each $\vec{v}, \exists -\vec{v}:\vec{v}+ -\vec{v}=0$
	**Distributive:** Covers both scalars and vectors 
		$\forall \alpha, \beta \in \mathbb{R} \text{ and all } \vec{x}, \vec{y} \in V$:
		1. $(\alpha + \beta) \cdot \vec{x}= \alpha \cdot \vec{x} + \beta \cdot \vec{x}$ 
		2. $\alpha(\vec{x}+\vec{y})=\alpha \cdot \vec{x} + \alpha \cdot\vec{y}$
	**Identity Scalar:** all vectors in the vector space multiplied by 1 is itself
		$1 \cdot\vec{x}=\vec{x}$

Note the vector and vector space definition is defined by its operations and is very general. This means we can treat a lot of things as vectors:
1. functions form an infinite dimensional vector space 
	1. infinite because we cannot capture all functions with a finite number of basis vectors
2. random variables
	1. random variables are functions that map from sample space to number space. We've established the functions are infinite dimensional vector spaces, so random variables are also infinite dimensional vector spaces

## Vector subspace
A vector subspace $S$ is a subset of the vector space $V$ that is closed by addition and multiplication under a scalar and satisfies the following properties
1. $\forall x, y \in S, x + y \in S$
2. $\forall x \in S, \lambda \in \mathbb{R}, \lambda \cdot x \in S$
3. $S$ is not empty and contains 0

Note that if $S$ is a subspace of $V$, $S$ is a vector space as well. 
### Examples
1. $\mathbb{R}^n$ is a subspace of $\mathbb{R}^n$
2. $\{0\}$ is a subspace of $\mathbb{R}^n$
3. any line $V$ going through the origin is a subspace of $\mathbb{R}^2$
4. is $\mathbb{R}^2 \textbackslash V$ a subspace of $\mathbb{R}^2$? Why?
	1. No, because it doesn't contain the zero set
5. What are the possible subspaces of $\mathbb{R}^2$?
	Either $V =\{0\} \text{ or } V \neq \{0\}$.
	If $V=\{0\}$, we can trivially show that it satisfies non-empty, addition, and scalar multiplication
	If $V \neq \{0\}$, $\exists v \in V \textbackslash  \{0\}$. 
		To satisfy non-emptiness and scalar multiplication, $\forall \lambda \in \mathbb{R}, \lambda v \in V$. This is just any line that passes through the origin. 
		So $V=\mathbb{R}v$
	If $V \neq \mathbb{R}v$?
		Then $\exists w \in V \textbackslash \mathbb{R}v$ such that $w \text{ and } v$ are independent. Thus they form a basis for $\mathbb{R}^2$. To be closed under addition and scalar multiplication, $V$ must be $\mathbb{R}^2$. 
		
# span and linear dependence
## linear combination
We say that $y \in V$ is a linear combination of the vectors $x_{1}, x_{2}, \dots, x_{k} \in V$ if $\exists \alpha_{1}, \alpha_{2}, \dots, \alpha_{k} \in \mathbb{R}$ such that $y=\sum_{i=1}^{k}\alpha_{i}x_{i}$. 

### properties
1. a linear combination is always a finite sum
2. if $S$ is a subspace of $V$, then any linear combination of vectors $x_{1},x_{2}, \dots, x_{k} \in S$ is also in $S$ 
## span
Let $x_{1}, x_{2}, \dots, x_{k}$ be vectors of $V$. The *linear span* of $x_{1}, x_{2},\dots, x_{k}$ is the set of all linear combinations of those vectors. We can understand it as the smallest subspace of $V$ that contains $x_{1}, x_{2}, \dots, x_{k}$
$Span(x_{1}, x_{2}, \dots, x_{k}) \coloneqq \{\alpha_{1}x_{1}+\alpha_{2}x_{2}+ \dots+\alpha_{k}x_{k} | \alpha_{1}, \dots, \alpha_{k} \in \mathbb{R}\}$

## linear dependence
The vectors $x_{1}, x_{2}, \dots, x_{k} \in V$ are called linearly dependent if there exists $\alpha_{1}, \alpha_{2}, \dots, \alpha_{k} \in \mathbb{R}$ that are **not all zero** such that $\alpha_{1}x_{1}+\alpha_{2}x_{2}+\dots+\alpha_{k}x_{k}=0$. They are linearly independent otherwise. 

### examples
1. is $(x,)$ linearly dependent?
	1. if x is the zero vector, then yes because alpha can be whatever 
	2. if x isn't the zero vector, then no because you can only get 0 by multiplying by 0
2. is $(x, -x)$ linearly dependent?
	1. no because $\alpha_{1}, \alpha_{2}$ can both be 1 to get a combination of 0

# basis and dimension
## basis
A family $x_{1}, x_{2}, \dots, x_{n}$ of vectors in $V$ is a basis of $V$ if 
1. $x_{1}, x_{2}, \dots, x_{n}$ are linearly independent
2. $Span(x_{1}, \dots, x_{n})=V$

Any vector space or subspace has a basis and is often infinite. 

## dimension
if $V$ is a vector space
1. if $V$ admits a basis $v_{1}, v_{2}, \dots, v_{n}$, then every basis of $V$ has also $n$ vectors. We say that $V$ has $n$ dimensions, denoted by $dim(V)=n$
2. otherwise, we say that $V$ has infinite dimension, demoted as $dim(V)=+  \infty$

### properties
Let $V$ be a vector space with dimension $n$ and let $x_{1}, x_{2}, \dots, x_{n} \in V$
1. if $x_{1},\dots, x_{n}$ are linearly independent, then $x_{1}, x_{2}, \dots, x_{n}$ is a basis of $V$. (we get span being V for free!)
2. if $Span(x_{1}, \dots, x_{n})=V$, then $(x_{1}, x_{2}, \dots, x_{n})$ is basis of V (we get linear independence for free!)

To show that a family of vectors $x_{1}, x_{2}, \dots, x_{k}$ forms a basis, we need to show 3 things
1. $k=n$
2. all vectors are linearly independent 
3. $Span(x_{1}, x_{2}, \dots, x_{k})=V$ 

Luckily, showing any 2 properties imply the 3rd one. 1 is normally easy to show, and we choose the easier between 2 and 3. 

### useful vocabulary
Let $S$ be a subspace of $\mathbb{R}^n$
1. if $dim(S)=1$, $S$ is a line
2. if $dim(S)=n-1$, $S$ is a hyperplane

## coordinates of a vector in a basis
If $v_{1}, v_{2}, \dots, v_{n}$ is a basis of $V$, then for every $x \in V$, there exists a unique vector  $(\alpha_{1}, \alpha_{2}, \dots, \alpha_{n})\in \mathbb{R}^n$ such that $x=\alpha_{1}v_{1}+ \dots + \alpha_{n}v_{n}$. We refer to $(\alpha_{1},\dots,\alpha_{n})$ as the coordinates of $x$. 

_Proof_:
The span of the set of vectors v is V since it is the basis of V, so x is within the span of v. Therefore there exists a vector alpha such that the linear combination of v and alpha is x. 
Suppose there exists another set of vectors beta such that x is a linear combination of v and beta, since x-x=0, and v is a basis so it is linearly independent, all coefficients for each v must be 0. Thus each alpha must equal each beta, thus the vectors alpha and beta are equal and there is a unique vector to get x. 


