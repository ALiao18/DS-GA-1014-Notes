2 useful formulas:
1. Let $x, y \in \mathbb{R}^{n}$. Then $\langle x,y  \rangle=x^{\top}y$
2. Let $A \in \mathbb{R}^{n \times m}, x \in \mathbb{R}^{m}, y \in \mathbb{R}^{n}$. Then $\langle Ax,y  \rangle=\langle x,A^{\top}y  \rangle$. This tells us that we can put symmetric matrices on either side and the answer stays the same. 
# table of contents
1. Gram-Schmidt algorithm
2. orthogonal matrices
3. eigenvalues and eigenvectors

---
# Gram-Schmidt algorithm

### purpose

How to create an orthonormal basis?

The Gram-Schmidt process takes as 
1. input: a linearly independent family $(x_{1},\dots,x_{k})$ of $\mathbb{R}^{n}$
2. output: an orthonormal basis $v_{1},\dots,v_{k}$ of $Span(x_{1},\dots,x_{k})$. 

Consequence: every subspace of $\mathbb{R}^{n}$ admits an orthonormal basis

So we need to do 2 things: normalize each vector, and remove the projection of each vector onto every other vector, in other words, project onto the complement of every other vector. Note that since take in a linearly independent family, we don't have 0s, so no need to worry about normalizing zeros. 

For example, let there be $x_{1}, x_{2}$. 
$$
\begin{align}
v_{1}&=\frac{x_{1}}{\lVert x_{1} \rVert } \\
\tilde{v_{2}}&=x_{2}-\langle x_{2},v_{1}  \rangle v_{1} \\
\langle \tilde{v_{2}},v_{1}  \rangle &= \langle x_{2}, v_{1} \rangle - \langle x_{2},v_{1}  \rangle \langle v_{1}, v_{1} \rangle = 0 \\
v_{2}&=\frac{\tilde{v_{2}}}{\lVert \tilde{v_{2}} \rVert }
\end{align}
$$
We know that $\tilde{v_{2}}$ cannot be 0 because it is 0 iff $v_{2} \not\perp v_{1}$. 

Using this, we can find $A=QR$ where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. 

## Gram-Schmidt construction

The Gram-Schmidt process is a recursive algorithm that constructs $v_{1},\dots,v_{k}$ in this order such that for all $i \in \{ 1,\dots,k \}$:
$$
\begin{align}
\mathcal{H}_{i}= \begin{cases}
(v_{1},\dots,v_{i}) \text{ is an orthogonal family} \\
Span(v_{1},\dots,v_{i}) = Span(x_{1},\dots,x_{i})
\end{cases}
\end{align}
$$
$\mathcal{H}_{1}$: $v_{1}$ is an orthonormal basis of $Span(x_{1})$ because $v_{1}=\frac{x_{1}}{\lVert x_{1} \rVert}$
$\mathcal{H}_{2}$: $(v_{1},v_{2})$... $Span(x_{1},x_{2})$ because $v_{2}=\alpha x_{1}+\alpha x_{2}$
and so on
$\mathcal{H}_{k}$: $(v_{1},\dots,v_{k})$ is an orthonormal basis of $Span(x_{1},\dots,x_{k})$.

# orthogonal matrices

A matrix $A \in \mathbb{R}^{n \times n}$ is called an orthogonal matrix if its columns are an orthonormal family. 

Proposition:
Let $A \in \mathbb{R}^{n \times n}$ The following points are equivalent:
1. $A$ is orthogonal
2. $A^{\top}A=Id_{n}$
3. $AA^{\top}=Id_{n}$
## orthogonal matrices and norm
Let $A \in \mathbb{R}^{n \times n}$ be an orthogonal matrix. Then A preserves the dot product in the sense that $\forall x, y \in \mathbb{R}^{n}$, 
$$
\begin{align}
\langle Ax,Ay  \rangle = \langle x,y  \rangle
\end{align}$$
In particular if we take $x=y$ we see that $A$ preserves the Euclidean norm: $\lVert Ax \rVert=\lVert x \rVert$

$$
\begin{align}
\langle Ax,Ay  \rangle &= (Ax)^{\top}(Ay) \\
&=x^{\top}A^{\top}Ay \\
&=x^{\top}y \\
&=\langle x,y  \rangle
\end{align}$$
### example: rotations and reflections
Lets say we have $x$ and rotate by angle $\theta$. 
$R_{\theta}=\begin{pmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{pmatrix}$
$$
\begin{cases}
cos^{2}\theta+sin^{2}\theta=1 \\
(-sin\theta)^{2}+cos^{2}\theta=1 \\
cos\theta(-sin\theta)+sin\theta \cdot cos\theta = 0
\end{cases}
$$

$S = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$

We can also show that these are the only types of orthogonal matrices to exist. 

## orthonormal bases
Let $(a_{1},a_{2},\dots,a_{n})$ an orthonormal basis of $\mathbb{R}^{n}$, and $A$ the $\mathbb{R}^{n \times n}$ matrix collecting the basis vectors in its columns.
Consider $x = (x_{1},x_{2},\dots,x_{n}) \in \mathbb{R}^{n}$ where $x_{1},\dots,x_{n}$ are the coordinates in the canonical basis of $\mathbb{R}^{n}$. 

The coordinates of $x$ in the $(a_{1},a_{2},\dots,a_{n})$ basis are given by $x'=A^{\top}x$.

Coordinates of $x$ in basis $a_{1},\dots,a_{n}$ are $(\langle x,a_{i}  \rangle)_{i=1,\dots,n}=(a_{i}^{\top}x)_{i=1,\dots,n}=A^{\top}x$.
This is not usually the case. Usually finding the orthonormal bases requires solving a linear system. It usually requires finding the inverse, so doing gaussian elimination. The general formula is $x'=A^{-1}x$, but $A^{-1}=A^{\top} \iff$ $A$ is an orthogonal matrix. 
# eigenvalues and eigenvectors

## Introduction
Note: diagonal matrices are easy to manipulate
Numerical: 
	Let $A$ be a product of simpler matrices

Geometric:
	Suppose we have a linear transformation $L: V \rightarrow V$.
	Is there a basis of V
## definition
Let $A \in \mathbb{R}^{n \times n}$. A non-zero vector $v \in \mathbb{R}^{n}$ is said to be an eigenvector of $A$ if $\exists \lambda \in \mathbb{R} \text{ s.t. } Av=\lambda v$. The scalar. $\lambda$ is then called an eigenvalue of $A$.

Examples: 
1. identity matrix:
	any (non-zero) vector $v$ is an eigenvector of $Id$ because $Id v=v$ for the eigenvalue $\lambda =1$.
	This is the only one 
2. matrix $A$ with $Ker(A) \neq \{ 0 \}$
	The eigenvector of $A$ for eigenvalue 0 are $Ker(A) / \{ 0 \}$. Which means 0 is an eigenvalue, meaning A is not invertible. **important!!!**

## matrix with no eigenvalues/vectors
How do we identify them?
## orthogonal projection
Let $P_{S}(x)$ be a orthogonal projection

- If $x \in S / \{ 0 \}$, $x$ is an eigenvector of $P_{S}$ with eigenvalue 1 ($P_{S}(s)=x$)
- If $x \in S^\perp / \{ 0 \}$, x is an eigenvector of $P_{S}$ with eigenvalue 0 ($P_{S}(x)=0$)
$Ker(P_{S})=S^\perp$
$S^\perp \subseteq Ker(P_{S})$: Let $x \in S^\perp$. $P_{S}(x)=\sum_{i=1}^{k}(v_{i}^{\top}x)v_{i}=0$