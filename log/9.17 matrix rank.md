*Motivation:*
Consider a dataset $x_{1},\dots,x_{n} \in \mathbb{R}^{d}$. What is its dimensionality?
There are many notions of dimensionality. The rank is one of them.
# table of contents
1. the rank
2. the rank-nullility theorem
3. rank and invertible matrices
4. transpose
--- 
# the rank

We can define the rank of a family $x_{1},\dots,x_{k}$ of vectors in $\mathbb{R}^{n}$ as the dimension of its span
$$
\begin{align}
rank(x_{1},\dots,x_{k})\stackrel{\text{def}}{=}dim(Span(x_{1},\dots,x_{k}))
\end{align}
$$
We know that the dimension cannot be larger than $n$ because the vector space is $\mathbb{R}^{n}$. More strictly, it cannot be greater than $k$ because all vectors in the span are linearly independent.

Some basic info about the rank:
1. $rank \in \mathbb{Z}$ 
2. $rank \leq min(k,n)$
## rank of a matrix
Let $M \in \mathbb{R}^{n \times m}$. Let $c_{1},\dots,c_{m} \in \mathbb{R}^{n}$ be its columns. We define
$$
\begin{align}
rank(M)&\stackrel{\text{def}}{=}rank(c_{1},\dots,c_{m}) \\
&=dim(\mathrm{Im}(M)) \\
&=dim(Span(c_{1},\dots,c_{m}))
\end{align}
$$

We can define the rank of the matrix by the rank of its vectors. This is equivalent to the dimension of the span of the vectors, which is then equivalent to the dimension of the image of M. 

What are its lower bounds with respect to Span? 
For a matrix M that is $n \times m$, as a generalization to the previous rank description, $rank(M) \leq min(m, n)$. 

For Example:
1. What is the rank of $Id_{n}$?
	$\mathrm{Im}(Id_{n})=Span(e_{1},\dots ,e_{n})=\mathbb{R}^{n}$
	$rank(Id_{n})=dim(\mathbb{R}^{n})=n$
2. What is the rank of $M=\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$. 
	1. we can find out by looking at if the vectors are linearly dependent, which can be done via gaussian elimination. 
	2. $rank=dim(Span(\begin{pmatrix} 1 \\ 3 \end{pmatrix}, \begin{pmatrix} 2 \\ 4 \end{pmatrix}))=2$
3. What is the rank of $M=\begin{pmatrix} 1 & 2 & 1 \\ 0 & 0 & 1 \end{pmatrix}$?
	1. Why must 1 be redundant even before checking independence? Because each vector in M is only in $\mathbb{R}^{2}$ but there are 3 vectors. Remember that rank is capped by the smaller of the row and column count. 

## rank of columns = rank of rows

$$
\begin{align}
\text{Let } M \in \mathbb{R}^{n \times m}&. \text{Let } x_{1},\dots,x_{n} \in \mathbb{R}^{m} \text{ be the rows of M and } c_{1},\dots,c_{m} \in \mathbb{R}^{n} \text{ be its columns} \\
rank(r_{1},\dots r_{n})&=rank(c_{1},\dots c_{m})= rank(M)
\end{align}
$$

## how to compute the rank?
For $v_{1},\dots,v_{k} \in \mathbb{R}^{n}$, and $\alpha \in \mathbb{R}$\ $\{ 0 \}, \beta \in \mathbb{R}$ we have 

$$
\operatorname{Span}(v_{1}, \ldots, v_{k}) = \begin{cases}
\operatorname{Span}(v_{1}, \ldots, v_{i-1}, \alpha v_{i}, v_{i+1}, \ldots, v_{k}) \\
\operatorname{Span}(v_{1}, \ldots, v_{i-1}, v_{i} + \beta v_{j}, v_{i+1}, \ldots, v_{k})
\end{cases}
$$

In simple terms, as long as you don't multiply a vector by 0 (compressing), linear combinations do not change the rank. As a consequence, gaussian elimination on columns keeps the image of a matrix unchanged

Example:
1. Compute rank of $A = \begin{pmatrix} 1 & -1 & 0 & 1 \\ 2 & 0 & 1 & -1 \\ -1 & 5 & 2 & 0 \end{pmatrix}$
# the rank-nullility theorem

Let $L: \mathbb{R}^{m}\to \mathbb{R}^{n}$ be a linear transformation. Then
$rank(L)+dim(Ker(L))=m$

Note:
1. *kernel/nullility* is all vectors that become 0 after the linear transformation. We can think about the kernel as *living in input subspace*
2. the *rank/image* is all vectors you can get after applying the linear transformation. We can think about the image as *living in output subspace*
	1. moreover, note that $rank(L)=dim(\mathrm{Im}(L))$
## proof
Goal: Within the input space, carve it into 2 orthogonal subspaces

Let $v_{1},\dots v_{k}$ a basis of $Ker(L)$. $(k=dim(Ker(L)))$
Complete it with $v_{k+1},\dots,v_{m}$ into a basis of $\mathbb{R}^{m}$.
$$
\begin{align}
\mathrm{Im}(L)&=\{ L(x),x \in \mathbb{R}^{m} \} \\
              &=\{ L(\alpha_{1}v_{1}+\dots+\alpha_{m}v_{m}), \alpha_{1},\dots,\alpha_{m} \in \mathbb{R} \} \\
              &= \{ \alpha_{1}L(v_{1})+\dots+\alpha_{k}L(v_{k})+\alpha_{k+1}L(v_{k+1})+\dots+\alpha_{m}L(v_{m}) | \alpha_{1}\dots, \alpha_{m} \in \mathbb{R}\} \\
              &=\{ \alpha_{k+1}L(v_{k+1})+\dots+\alpha_{m}L(v_{m}) | \alpha_{k+1},\dots,\alpha_{m} \in \mathbb{R} \} \\
              &=Span(L(v_{k+1}),\dots,L(v_{m})) \quad \quad \text{.     m-k vectors}
\end{align}
$$
We know that $L_{1},\dots,L_{k}$ is linearly independent as it is the basis. We also know the $v_{k+1},\dots,v_{m}$ is linearly independent. However, it is not immediately obvious that $L(v_{1}),\dots,L(v_{k})$ is linearly independent as well.

*Let $v_{1}$ show that $L(v_{k+1}),\dots,L(v_{m})$ are linearly independent by contradiction*
Assume that $\exists \alpha_{k+1},\dots,\alpha_{m} \in \mathbb{R}^{}$ not all zero such that $\alpha_{k+1}L(v_{k+1})+\dots+\alpha_{m}L(v_{m})=0$.
So, $L(\alpha_{k+1}v_{k+1}+\dots+\alpha_{m}v_{m})=0$
By definition, that is in $Ker(L)$. Contradiction because $Ker(L)=Span(v_{1},\dots,v_{k})$
So, $\alpha_{k+1}v_{k+1}+\dots+\alpha_{m}v_{m}=\alpha_{1}v_{1}+\dots+\alpha_{k}v_{k}$ for some $\alpha_{1},\dots,\alpha_{k} \in \mathbb{R}$.
Then, $-\alpha_{1}v_{1}+\dots + -\alpha_{k}v_{k}+\alpha_{k+1}v_{k+1}+\dots+\alpha_{m}v_{m} = 0$
## Inequalities
Let $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{m \times k}$. Then the following holes
1. $rank(A)\leq min(n, m)$
2. $rank(AB) \leq min(rank(A), rank(B))$

Each multiplication can only introduce more dependencies, there is necessary information loss, so you can only go down in rank and not up. 
### proof of inequality 2:
1. showing $rank(AB)\leq rank(A)$
$rank(AB) \leq rank(A)$
$dim(\mathrm{Im}(AB)) \subseteq dim(\mathrm{Im}(A))$?
$\mathrm{Im}(AB)=\{ ABx, x \in \mathbb{R}^{k} \} \subseteq \mathrm{Im}(A)=\{ Az, z \in \mathbb{R}^{m} \}$
Let $y \in \mathrm{Im}(AB)$ Then $y = ABx$ for some $x \in \mathbb{R}^{k}$. Let $z=Bx$. Then $y=Az \in \mathrm{Im}(A)$. So, $\mathrm{Im}(AB) \subseteq \mathrm{Im}(A)$. 

2. showing $rank(AB)\leq rank(B)$
Since AB and B no longer share a output space but they do share an input space, we look at the kernel. 

By the rank-nullility theorem, we have 
$$
\begin{align}
rank(B)&=m-dim(Ker(B)) \\
rank(AB)&=m-dim(Ker(AB)) \\
\therefore \text{ to show that } rank(AB) \leq rank(B), &\text{ we need to show } Ker(B) \subseteq Ker(AB) \} \\
\text{Let } x \in Ker(B), &\text{ by definition }Bx=0.  \\
\text{Now, suppose we do } &ABx.  \\
\text{By the associative property of matrix multiplication, } &ABx=A(Bx)=A(0)=0 \\
\therefore \text{if } x \in Ker(B), &x \in Ker(AB). \\
\therefore Ker(B) &\subseteq Ker(AB) \\
\therefore dim(Ker(B))&\leq dim(Ker(AB)) \\
\text{from rank nullility theorem, we have } &rank(AB) \leq rank(B)
\end{align}
$$


# rank of invertible matrices
Theorem:
Let $M \in \mathbb{R}^{n \times n}$. The following points are equivalent:
1. $M$ is invertible
2. $rank(M)=n$
3. $Ker(M)=\{ 0 \}$
4. $\forall y \in \mathbb{R}^{n}, \exists \text{ unique } x \in \mathbb{R}^{n} \text{ s.t. } Mx=y$

## $1 \rightarrow 2$
Let $y \in \mathbb{R}^{n} \text{ s.t. } y=Mx$
$M(M^{-1}y)=(MM^{-1})y=y \therefore y \in \mathrm{Im}(M)$
$\therefore \mathrm{Im}(M) \in \mathbb{R}^{n}, \therefore rank(M)=n$.
## $2 \rightarrow 3$
From the rank nullility theorem, we know that $dim(Ker(M))=n-rank(M)$
We've demonstrated that $rank(M)=n, \therefore dim(Ker(M))=0$
## $3 \rightarrow 4$
Assume that $Ker(M)=\{ 0 \}$ Let $y \in \mathbb{R}^{n}$
Let us show that $\exists$ a unique $x \in \mathbb{R}^{n} \text{ s.t. } y = Mx$
*existence:* equivalent to $y \in \mathrm{Im}(M)$
By rank nullility theorem: $dim(\mathrm{Im}(M)) - n - dim(Ker(M))=n$
so $\mathrm{Im}(M)=\mathbb{R}^{n}$ so $y \in \mathrm{Im}(M)$
*unicity:* Assume that $y=Mx=Mx' \text{ s.t. } x, x' \in \mathbb{R}^{n}$
$M(x-x')=Mx-Mx'=y-y=0$
So $x-x' \in Ker(M)=\{ 0 \}$ so $x-x'=0, x=x'$
## $4 \rightarrow 1$
We see from property 4 that $M$ is a bijection, meaning there is a unique mapping for all values in the subspace. To prove property 1 we want to show $\exists M^{-1} \text{ s.t. } MM^{-1}=Id_{n}$.
By the existence proof in part 4, we know that for each e in the canonical basis of $\mathbb{R}^{n}, \exists \text{unique vector }v \text{ s.t } Mv=e$.
Let $M^{-1} = [v_{1},\dots, v_{n}]$
$MM^{-1}=[Mv_{1},Mv_{2},\dots,Mv_{n}]=[e_{1},\dots, e_{n}]=Id_{n}$
Since $MM^{-1}=Id_{n},$ M is invertible.
# transpose of a matrix

Let $M \in \mathbb{R}^{ n \times m}$ We define its transpose $M^{\top}\in \mathbb{R}^{m \times n }$ by $M^{\top}_{i,j}=M_{j,i} \forall i \in \{ 1,\dots,m \}$ and $j \in \{ 1,\dots,n \}$
1. $(M^{\top})^{\top}=M$
2. The mapping $M \mapsto M^{\top}$ is linear, so its closed under addition and scalar multiplication

## Properties
1. $\forall A \in \mathbb{R}^{n \times m}, rank(A)=rank(A^{\top})$
	1. rank of A = rank of column of A = rank of rows of A transpose = rank of columns of A transpose = rank of A transpose
2. let $A \in \mathbb{R}^{n \times m}, B \in \mathbb{R}^{m \times k}, (AB)^{\top}=B^{\top}A^{\top} \neq A^{\top}B^{\top}$.
	1. $((AB)^{\top})_{ij}=(AB)_{ji}=\sum_{l=1}^{m}A_{jl}B_{li}=\sum_{l=1}^{m}A^{\top}_{lj}B^{\top}_{li}=(B^{\top}A^{\top})_{ij}$

A square matrix $A \in \mathbb{R}^{n \times n}$ is said to be symmetric if $\forall i, j \in  \{ 1, \dots,n \},A_{i,j}=A_{j,i}$. Or, $A = A^{\top}$
$\forall M \in \mathbb{R}^{n \times m}$ the matrix $MM^{\top}$ is symmetric

